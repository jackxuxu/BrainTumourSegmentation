{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kevinteng/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "BATCH_SIZE = 16\n",
    "lr = 0.000001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = '06' #save version \n",
    "dropout=0.2 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/HGG/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1e-5):\n",
    "    '''\n",
    "    Dice coefficient for tensorflow\n",
    "    :param y_true: Ground truth\n",
    "    :param y_pred: Prediction from the model\n",
    "    :return: dice coefficient \n",
    "    '''\n",
    "    #if input is not flatten\n",
    "    if (tf.rank(y_true)!=1 and tf.rank(y_pred)!=1):\n",
    "        y_true = tf.reshape(y_true, [-1]) #flatten \n",
    "        y_pred = tf.reshape(y_pred, [-1]) #flatten\n",
    "    #casting for label from int32 to float32 for computation\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2.0 * intersection + smooth) / \\\n",
    "(tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def ss_metric(y_true, y_pred, label_type = 'binary', mode = 'global', smooth=1e-5):\n",
    "    '''\n",
    "    Compute sensitivity and specificity for groundtruth and prediction\n",
    "    :param y_true: Ground truth\n",
    "    :param y_pred: Prediction from the model\n",
    "    :label_type: 'binary': input labels is binarized\n",
    "                 'multi': mutli class labels\n",
    "    :mode: 'local' compute the sensitivity label wise\n",
    "           'global' compute the sensitivity overall\n",
    "    :return: sensitivity & specificity \n",
    "    '''\n",
    "    #if input is not flatten\n",
    "    if (tf.rank(y_true)!=1 and tf.rank(y_pred)!=1):\n",
    "        y_true = tf.reshape(y_true, [-1]) #flatten \n",
    "        y_pred = tf.reshape(y_pred, [-1]) #flatten\n",
    "    #label types    \n",
    "    if label_type =='binary':\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true , y_pred, labels = [0,1]).ravel()\n",
    "        sensitivity = (tp+smooth)/(tp+fn+smooth)\n",
    "        specificity = (tn+smooth)/(tn+fp+smooth)\n",
    "    if label_type =='multi':\n",
    "        cm = confusion_matrix(y_true , y_pred, labels = [0,1,2,3])\n",
    "        #true positive rate \n",
    "        if mode=='global':\n",
    "            tp = np.trace(cm)\n",
    "            tp_fn = np.sum(cm)\n",
    "        else: #local\n",
    "            tp = np.diag(cm)\n",
    "            tp_fn = np.sum(cm,1)\n",
    "        sensitivity = (tp+smooth)/(tp_fn+smooth)\n",
    "        #true negative rate\n",
    "        diag = np.diag(cm)\n",
    "        tn = []\n",
    "        for i in range(len(cm)):\n",
    "            negs = np.sum([neg for neg in diag if neg!=diag[i]]) \n",
    "            tn.append(negs)\n",
    "        cm_copy = cm\n",
    "        #make diagonal 0\n",
    "        for i in range(len(cm)):\n",
    "            for j in range(len(cm)):\n",
    "                if i==j:\n",
    "                    cm_copy[i,j]=0\n",
    "        if mode=='global':\n",
    "            tn = np.sum(tn)\n",
    "            fp = np.sum(cm_copy)\n",
    "        else: #local\n",
    "            tn = np.array(tn)\n",
    "            fp = np.sum(cm_copy, 0)\n",
    "        specificity = (tn+smooth)/(tn+fp+smooth)\n",
    "    return sensitivity, specificity\n",
    "\n",
    "def compute_metric(y_true, y_pred, label_type='binary'):\n",
    "    '''\n",
    "    This function compute the metrics specify by BraTS competition\n",
    "    which is dice coefficient, sensitivity, specificity\n",
    "    :param y_true: Ground truth image\n",
    "    :param y_pred: Prediction image from the model\n",
    "    :label_type: 'binary': input labels is binarized\n",
    "             'multi': mutli class labels\n",
    "    :return: dice coefficient, sensitivity & specificity list\n",
    "            with order ['core', 'enhancing', 'complete']\n",
    "    '''\n",
    "    y_list = [y_true, y_pred]\n",
    "    tumours = ['core', 'enhancing', 'complete']\n",
    "    dc_output = []\n",
    "    sens_output = []\n",
    "    spec_output = []\n",
    "    #compute dice coefficient for each tumour type\n",
    "    for tumour_type in tumours:\n",
    "        if label_type =='multi':\n",
    "            #label 1, 3(4)\n",
    "            if tumour_type== 'core':\n",
    "                y_true, y_pred = [np.where(((lbl==1) | (lbl==3)), lbl, 0) for lbl in y_list]\n",
    "            #label 3(4)\n",
    "            if tumour_type== 'enhancing':\n",
    "                y_true, y_pred = [np.where(lbl==3, lbl, 0) for lbl in y_list]\n",
    "            #label 1,2,3,\n",
    "            if tumour_type== 'complete':\n",
    "                y_true, y_pred = [np.where(lbl>=0, lbl, 0) for lbl in y_list]\n",
    "        if label_type =='binary':\n",
    "            #label 1, 3(4) =>1\n",
    "            if tumour_type== 'core':\n",
    "                y_true, y_pred = [np.where(((lbl==1) | (lbl==3)), 1, 0) for lbl in y_list]\n",
    "            #label 3(4) =>1\n",
    "            if tumour_type== 'enhancing':\n",
    "                y_true, y_pred = [np.where(lbl==3, 1, 0) for lbl in y_list]\n",
    "            #label 1,2,3 =>1\n",
    "            if tumour_type== 'complete':\n",
    "                y_true, y_pred = [np.where(lbl>=0, 1, 0) for lbl in y_list]\n",
    "        dc_list = []\n",
    "        sens_list = []\n",
    "        spec_list = []\n",
    "        for idx in range(len(y_true)): \n",
    "            \n",
    "            y_true_f= tf.reshape(y_true[idx], [-1]) #flatten \n",
    "            y_pred_f = tf.reshape(y_pred[idx], [-1]) #flatten\n",
    "\n",
    "            dc = dice_coef(y_true_f, y_pred_f)\n",
    "            sensitivity, specificity = ss_metric(y_true_f, y_pred_f)    \n",
    "            #store values\n",
    "            dc_list.append(dc)\n",
    "            sens_list.append(sensitivity)\n",
    "            spec_list.append(specificity)\n",
    "        #output [BATCH_SIZE, tumours_type]\n",
    "        #taking the mean along the batch axis\n",
    "        mean_ = lambda x: np.mean(x)\n",
    "        dc_batch_mean = mean_(dc_list)\n",
    "        sens_batch_mean = mean_(sens_list)\n",
    "        spec_batch_mean = mean_(spec_list)\n",
    "        #append for each tumour type\n",
    "        dc_output.append(dc_batch_mean)\n",
    "        sens_output.append(sens_batch_mean)\n",
    "        spec_output.append(spec_batch_mean)\n",
    "    #for each list the order is as following=> 'core','enhancing','complete'    \n",
    "    return dc_output, sens_output, spec_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import conv_block, coordconv_block, up, pool\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "\n",
    "def Unet_model(input_layer):\n",
    "    #downsampling\n",
    "#     conv1 = coordconv_block(input_layer, x_dim=240, y_dim=240, filters=64)\n",
    "    conv1 = conv_block(input_layer, filters=64, kernel_initializer=hn)\n",
    "    pool1 = pool(conv1)\n",
    "    \n",
    "    conv2 = conv_block(pool1, filters=128, kernel_initializer=hn)\n",
    "    pool2 = pool(conv2)\n",
    "    \n",
    "    conv3 = conv_block(pool2, filters=256, kernel_initializer=hn)\n",
    "    pool3 = pool(conv3)\n",
    "    \n",
    "    conv4 = conv_block(pool3, filters=512, kernel_initializer=hn, dropout_rate = dropout)\n",
    "    pool4 = pool(conv4)\n",
    "    \n",
    "    conv5 = conv_block(pool4, filters=1024, kernel_initializer=hn, dropout_rate = dropout)\n",
    "    \n",
    "    #upsampling\n",
    "    up1 = up(conv5,filters=512, merge=conv4, kernel_initializer=hn)\n",
    "#     conv6 = coordconv_block(up1, x_dim=30, y_dim=30, filters=512)\n",
    "    conv6 = conv_block(up1, filters=512, kernel_initializer=hn)\n",
    "    \n",
    "    up2 = up(conv6, filters=256, merge=conv3, kernel_initializer=hn)\n",
    "    conv7 = conv_block(up2, filters=256, kernel_initializer=hn)\n",
    "    \n",
    "    up3 = up(conv7, filters=128, merge=conv2, kernel_initializer=hn)\n",
    "    conv8 = conv_block(up3, filters=128, kernel_initializer=hn)\n",
    "    \n",
    "    up4 = up(conv8, filters=64, merge=conv1, kernel_initializer=hn)\n",
    "    conv9 = conv_block(up4, filters=64, kernel_initializer=hn)\n",
    "    \n",
    "    output_layer = Conv2D(4, (1,1), activation = 'softmax')(conv9)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(240,240,4))\n",
    "Unet = Model(input_layer, Unet_model(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do..Sensitivity\n",
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = Unet(image)\n",
    "        loss = xent(label, model_output)\n",
    "    gradients = tape.gradient(loss, Unet.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, Unet.trainable_variables))\n",
    "    \n",
    "    return model_output, loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "max_epochs = 30\n",
    "#list\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "loss_inner = []\n",
    "while epochs <= max_epochs:\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    for tf_re in sorted(os.listdir(tfrecords_read_dir)):\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        acc_inner = []\n",
    "        for imgs in dataset:\n",
    "            image = imgs[:,:,:,:4]\n",
    "            #unprocessed label for plotting \n",
    "            label = imgs[:,:,:,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            img_seg, loss, gradients = train_fn(image, label) #training function \n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32) \n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            dc_list, sens_list, spec_list =compute_metric(label,img_seg)\n",
    "            #append accuracy for every steps\n",
    "            acc_inner.append(acc)\n",
    "            if epochs%5==0:\n",
    "                dc_app.append(dc_list)\n",
    "                sens_app.append(sens_list)\n",
    "                spec_app.append(spec_list)\n",
    "            #output\n",
    "            if steps%2000==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                loss_list.append(loss)\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp, sens_list_stp, spec_list_stp =compute_metric(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Sensitivity: {}\".format(sens_list_stp))\n",
    "                print(\"Specificity: {}\".format(spec_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "        acc_list.append(np.mean(acc_inner))\n",
    "    if epochs%5==0:\n",
    "        mean_dc = np.mean(np.array(dc_app),0)\n",
    "        mean_sens = np.mean(np.array(sens_app),0)\n",
    "        mean_spec = np.mean(np.array(spec_app),0)\n",
    "        print()\n",
    "        print('-----------<Summary for Epoch:{}>------------'.format(epochs))\n",
    "        print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "        print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "        print(\"Mean Sensitivity: {}\".format(mean_sens))\n",
    "        print(\"Mean Specificity: {}\".format(mean_spec))\n",
    "        print('------------------------------------------------')\n",
    "        print()\n",
    "    epochs+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet.save_weights('/home/kevinteng/Desktop/model_weights/Unet_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fn(image, label):\n",
    "    Unet.load_weights('/home/kevinteng/Desktop/model_weights/Unet_{}.h5'.format(ver))\n",
    "    Unet.trainable = False\n",
    "    model_output = Unet(image)\n",
    "    loss = xent(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfrecords_val = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/LGG/'\n",
    "\n",
    "steps = 1\n",
    "acc_list = []\n",
    "for tf_re in sorted(os.listdir(tfrecords_val)):\n",
    "    tf_dir = os.path.join(tfrecords_val+tf_re)\n",
    "    dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    for imgs in dataset:\n",
    "        image = imgs[:,:,:,:4]\n",
    "        label = imgs[:,:,:,-1]\n",
    "        label = tf.where(label==4,3,label)\n",
    "        #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "        label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "        img_seg, loss = val_fn(image, label) #validation function \n",
    "        #map from sparse to label\n",
    "        img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32) \n",
    "        label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "        #accuracy of the output values for that batch\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "        dc_list, sens_list, spec_list =compute_metric(label,img_seg)\n",
    "        #append\n",
    "        acc_list.append(acc)\n",
    "        dc_app.append(dc_list)\n",
    "        sens_app.append(sens_list)\n",
    "        spec_app.append(spec_list)\n",
    "        #output\n",
    "        if steps%100==0:\n",
    "#             dc_list, sens_list, spec_list =compute_metric(label[0],img_seg[0])\n",
    "            input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "            caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "            plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "            acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "            dc_list, sens_list, spec_list =compute_metric(label[0],img_seg[0])\n",
    "            print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "            print(\"Accuracy: {}\".format(acc_stp))\n",
    "            print(\"Dice coefficient: {}\".format(dc_list))\n",
    "            print(\"Sensitivity: {}\".format(sens_list))\n",
    "            print(\"Specificity: {}\".format(spec_list))\n",
    "        steps+=1\n",
    "    mean_dc = np.mean(np.array(dc_app),0)\n",
    "    mean_sens = np.mean(np.array(sens_app),0)\n",
    "    mean_spec = np.mean(np.array(spec_app),0)\n",
    "    print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "    print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "    print(\"Mean Sensitivity: {}\".format(mean_sens))\n",
    "    print(\"Mean Specificity: {}\".format(mean_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
