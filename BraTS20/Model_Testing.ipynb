{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from utils_model import *\n",
    "from tensorflow.keras.layers import Conv2D, Add, Multiply\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Softmax, Input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PAM(inp_feature, layer_name, kernel_initializer='glorot_uniform', acti='relu'):\n",
    "    '''\n",
    "    Position attention module\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param layer_name: List of layer names\n",
    "    [1st conv block, 2nd conv block, softmax output, 3rd conv block, position coefficient, Add output]\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]\n",
    "    :return: PAM features [w/4,h/4,c]\n",
    "    '''\n",
    "    # dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    # scale down ratio\n",
    "    c_8 = c//8\n",
    "    #\n",
    "    assert len(layer_name)>=5, 'Layer list length should be 5!'\n",
    "    # Branch01 Dimension: [w,h,c/8] => [(wxh),c/8]\n",
    "    query = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[0], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "            kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    query = tf.reshape(query,[-1,(w*h),c_8 ])\n",
    "    # Branch02 Dimension: [w,h,c/8] => [c/8,(wxh)]\n",
    "    key = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[1], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    key = tf.reshape(key, [-1,(w*h),c_8 ])\n",
    "    key = tf.einsum('bij->bji', key) # transpose/permutation\n",
    "    # matmul pipeline 01 & 02\n",
    "    matmul_0102 = tf.einsum('bij,bjk->bik', query, key) # [(wxh),(wxh)]\n",
    "    #attention coefficient\n",
    "    alpha_p = Softmax(name=layer_name[2])(matmul_0102) # [(wxh),(wxh)]\n",
    "    # Branch03\n",
    "    value = conv_2d(inp_feature, filters=c, layer_name=layer_name[3], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    value = tf.reshape(value,[-1,(w*h),c]) # [(wxh),c]\n",
    "    matmul_all = tf.einsum('bij,bjk->bik',alpha_p,value) # [(wxh),c]\n",
    "    # Output\n",
    "    output = tf.reshape(matmul_all, [-1,w,h,c]) # [w,h,c]\n",
    "    # learnable coefficient to control the importance of CAM\n",
    "    lambda_p = Conv2D(filters=1,kernel_size=1, padding='same',activation='sigmoid', name=layer_name[4])(inp_feature)\n",
    "    output = Multiply()([output, lambda_p])\n",
    "    output_add = Add(name = layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def CAM(inp_feature, layer_name):\n",
    "    '''\n",
    "    Channel attention module\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]k\n",
    "    :param layer_name: List of layer names\n",
    "        [softmax output, channel attention coefficients, Add output]\n",
    "    :return: CAM features [w/4,h/4,c]\n",
    "    '''\n",
    "    # dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    # learnable coefficient to control the importance of CAM\n",
    "    assert len(layer_name)>=2, 'Layer list length should be 2!'\n",
    "    # Branch01 Dimension: [w,h,c] => [(wxh),c]\n",
    "    query = tf.reshape(inp_feature, [-1,(w*h),c])\n",
    "    # Branch02 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    key = tf.reshape(inp_feature, [-1,(w*h),c]) # [(wxh),c]\n",
    "    key = tf.einsum('ijk->ikj', key) # Permute:[c,(wxh)]\n",
    "    # matmul pipeline 01 & 02\n",
    "    matmul_0201 = tf.einsum('ijk,ikl->ijl', key, query) # [c,c]\n",
    "    #attention coefficient\n",
    "    alpha_c = Softmax(name=layer_name[0])(matmul_0201) # [c,c]\n",
    "    # Branch03 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    value = tf.reshape(inp_feature,[-1,(w*h),c]) # [(wxh),c]\n",
    "    matmul_all = tf.einsum('ijk,ikl->ijl', value, alpha_c) # [(wxh),c]\n",
    "    # output\n",
    "    output = tf.reshape(matmul_all,[-1,w,h,c])# [w,h,c]\n",
    "    #\n",
    "    lambda_c = tf.keras.backend.variable(tf.zeros([1]), dtype='float32')\n",
    "    output = Multiply()([output, lambda_c])\n",
    "    output_add = Add(name=layer_name[-1])([output, inp_feature])\n",
    "    return output_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "class att_var(layers.Layer):\n",
    "    '''\n",
    "    Attention variable\n",
    "    '''\n",
    "    def __init__(self, initial_val):\n",
    "        super(att_var, self).__init__()\n",
    "        self.initial_val = initial_val\n",
    "    def __call__(self):\n",
    "        lambda_ = tf.Variable(initial_value=self.initial_val, trainable=True)\n",
    "        return lambda_\n",
    "\n",
    "lambda_c = att_var(tf.zeros([1]))\n",
    "tst = lambda_c()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hn = 'he_normal' #kernel initializer\n",
    "lambda_ = tf.keras.backend.variable(tf.zeros([1]), dtype='float32')\n",
    "input_layer = Input(shape=(200,200,128))\n",
    "model = Model(input_layer, CAM(input_layer, layer_name_ga[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 200, 200, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_25 (TensorF [(None, 40000, 128)] 0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_18 (TensorFl [(None, 128, 40000)] 0           tf_op_layer_Reshape_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_24 (TensorF [(None, 40000, 128)] 0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_19 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_18[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_26 (TensorF [(None, 40000, 128)] 0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock01 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_19[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_20 (TensorFl [(None, 40000, 128)] 0           tf_op_layer_Reshape_26[0][0]     \n",
      "                                                                 cam01_softmaxblock01[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_27 (TensorF [(None, 200, 200, 12 0           tf_op_layer_Einsum_20[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_4 (TensorFlowOp [(None, 200, 200, 12 0           tf_op_layer_Reshape_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock01 (Add)          (None, 200, 200, 128 0           tf_op_layer_Mul_4[0][0]          \n",
      "                                                                 input_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}