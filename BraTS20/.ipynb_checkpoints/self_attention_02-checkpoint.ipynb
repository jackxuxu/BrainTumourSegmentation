{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from utils import compute_metric_dc\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue => Label 1 (Necrotic and Non-enhancing Tumor Core)\n",
    "- Yellow => Label 2 (Peritumoral Edema)\n",
    "- Green => Label 3/4 (GD-Enhancing Tumor)\n",
    "---\n",
    "* Core => Label 1 & 3\n",
    "* Enhancing => Label 3\n",
    "* Complete => Label 1,2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "max_epochs = 20\n",
    "BATCH_SIZE = 8\n",
    "lr = 0.001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = 'model_self_attention_02' #save version \n",
    "dropout=0.3 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/'\n",
    "stack_npy = \"/home/kevinteng/Desktop/ssd02/BraTS2020_stack03/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def generalized_dice(y_true, y_pred, smooth = 1e-5):\n",
    "    \"\"\"\n",
    "    Generalized Dice Score\n",
    "    https://arxiv.org/pdf/1707.03237\n",
    "    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true    = tf.reshape(y_true,shape=(-1,4))\n",
    "    y_pred    = tf.reshape(y_pred,shape=(-1,4))\n",
    "    sum_p     = tf.reduce_sum(y_pred, -2)\n",
    "    sum_r     = tf.reduce_sum(y_true, -2)\n",
    "    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)\n",
    "    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)\n",
    "    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))\n",
    "    return generalized_dice\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred):   \n",
    "    return 1-generalized_dice(y_true, y_pred)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    The final loss function consists of the summation of two losses \"GDL\" and \"CE\"\n",
    "    with a regularization term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)\n",
    "\n",
    "def data_aug(imgs, seed=8888):\n",
    "    x = tf.image.random_flip_up_down(imgs,seed)\n",
    "    x = tf.image.random_flip_left_right(x,seed)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Layer Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# template for guided attention block\n",
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import conv_block, coordconv_block, up, pool, attention_block\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Activation, Add, Multiply, GaussianNoise\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, Dropout, Softmax, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Layer, Softmax\n",
    "from coord_conv import CoordConv\n",
    "from utils_model import *\n",
    "from attention import *\n",
    "\n",
    "def guided_attention_block(inp_feature, layer_name_p, layer_name_c):\n",
    "    '''\n",
    "    Guided attention block that takes feature as input and concatenates features\n",
    "    from PAM and CAM as output\n",
    "    :param inp_feature: Input features\n",
    "    :param layer_name_p: layer name list for PAM\n",
    "    :param layer_name_c: layer name list for CAM\n",
    "    :return: squeezed concatenated features of PAM and CAM\n",
    "    '''\n",
    "    pam_feature = PAM(inp_feature, layer_name_p, kernel_initializer=hn)\n",
    "    cam_feature = CAM(inp_feature, layer_name_c)\n",
    "    add = Add()([pam_feature,cam_feature]) #[60,60,128]\n",
    "    up = UpSampling2D(size=(4,4))(add) #[240,240,128]\n",
    "    squeeze = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn,\n",
    "                       activation='relu')(up)\n",
    "    return squeeze\n",
    "\n",
    "def guided_attention(res_feature, ms_feature, layer_name):\n",
    "    '''\n",
    "    Guided attention module\n",
    "    :param res_feature: Upsampled Feature maps from Res Block\n",
    "    :param ms_feature: Multi scale feature maps result from Res Block\n",
    "    :param layer_name: Layer Name should consist be a list contating 4 list\n",
    "    Example:\n",
    "    layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                      'pam01_alpha','pam01_add']\n",
    "    layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "    layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                      'pam02_alpha', 'pam02_add']\n",
    "    layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "    layer_name = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "    :return: guided attention module with shape same as input\n",
    "    '''\n",
    "    assert len(layer_name)==4, \"Layer name should be a list consisting 4 lists!\"\n",
    "    #self attention block01\n",
    "    concat01 = concatenate([res_feature, ms_feature], axis=-1)\n",
    "    squeeze01 = guided_attention_block(concat01, layer_name[0], layer_name[1])\n",
    "    multi01 = Multiply()([squeeze01, ms_feature])\n",
    "    #self attention block02\n",
    "#     concat02 = concatenate([multi01, res_feature],axis=-1)\n",
    "#     squeeze02 = guided_attention_block(concat02, layer_name[2], layer_name[3])\n",
    "    return multi01\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Resnet as backbone for multiscale feature retrieval.\n",
    "    Each resblock output(input signal), next resblock output(gated signal) is\n",
    "    feed into the gated attention for multi scale feature refinement.\n",
    "    Each gated attention output is pass through a bottle neck layer to standardize\n",
    "    the channel size by squashing them to desired filter size of 64.\n",
    "    The features are upsampled at each block to the corresponding [wxh] dimension\n",
    "    of w:240, h:240.\n",
    "    The upsampled features are concat and squash to corresponding channel size of 64\n",
    "    which yield multiscale feature.\n",
    "    :param x: batched images\n",
    "    :return: feature maps of each res block\n",
    "    '''\n",
    "    #inject noise\n",
    "    gauss1 = GaussianNoise(0.01)(x)\n",
    "    #---- ResNet and Multiscale Features----\n",
    "    #1st block\n",
    "    conv01 = CoordConv(x_dim=240, y_dim=240, with_r=False, filters=64, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=hn, name='conv01')(gauss1)\n",
    "    res_block01 = res_block_sep(conv01, filters=[128, 64], layer_name=[\"conv02\", \"conv03\", \"add01\"])\n",
    "    #2nd block\n",
    "    down_01 = down_sampling_sep(res_block01, filters=128, layer_name = 'down_01',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=120, y_dim=120)\n",
    "    res_block02 = res_block_sep(down_01, filters=[256, 128], layer_name=[\"conv04\", \"conv05\", \"add02\"])\n",
    "    #3rd block\n",
    "    down_02 = down_sampling_sep(res_block02, filters=256, layer_name = 'down_02',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=60, y_dim=60)\n",
    "    res_block03 = res_block_sep(down_02, filters=[512, 256], layer_name=[\"conv06\", \"conv07\", \"add03\"])\n",
    "    #4th block\n",
    "    down_03 = down_sampling_sep(res_block03, filters=512, layer_name = 'down_03',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=30, y_dim=30)\n",
    "    res_block04 = res_block_sep(down_03, filters=[1024, 512], layer_name=[\"conv08\", \"conv09\", \"add04\"])\n",
    "    #grid attention blocks\n",
    "    att_block01 = attention_block(res_block01,res_block02,64,'grid_att01')\n",
    "    att_block02 = attention_block(res_block02,res_block03,128,'grid_att02')\n",
    "    att_block03 = attention_block(res_block03, res_block04,256,'gird_att03')\n",
    "    #bottle neck => layer squash all attention block to same filter size 64\n",
    "    bottle01 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block01)\n",
    "    bottle02 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block02)\n",
    "    bottle03 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block03)\n",
    "    #upsampling for all layers to same (wxh) dimension=>240x240\n",
    "    up01 = bottle01 #[240,240,64]\n",
    "    up02 = UpSampling2D(size=(2, 2), interpolation='bilinear')(bottle02) #[120,120,64]=>[240,240,64]\n",
    "    up03 = UpSampling2D(size=(4,4), interpolation='bilinear')(bottle03) #[60,60,64]=>[240,240,64]\n",
    "    #multiscale features\n",
    "    concat_all = concatenate([up01,up02,up03],axis=-1) #[240,240,3*64]\n",
    "    #squeeze to have the same channel as upsampled features [240,240,3*64] => [240,240,64]\n",
    "    ms_feature = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(concat_all)\n",
    "    #Segmentations from multiscale features *without softmax activation\n",
    "    seg_01 = Conv2D(4, (1,1), name='seg_01')(up01)\n",
    "    seg_02 = Conv2D(4, (1,1), name='seg_02')(up02)\n",
    "    seg_03 = Conv2D(4, (1,1), name='seg_03')(up02)\n",
    "\n",
    "    #----self guided attention blocks-----\n",
    "    ga_01 = guided_attention(up01, ms_feature, layer_name_ga[0])\n",
    "    ga_02 = guided_attention(up02, ms_feature, layer_name_ga[1])\n",
    "    ga_03 = guided_attention(up03, ms_feature, layer_name_ga[2])\n",
    "    #Segmentations from guided attention features *without softmax activation\n",
    "    seg_ga01 = Conv2D(4, (1,1), name='seg_ga01')(ga_01)\n",
    "    seg_ga02 = Conv2D(4, (1,1), name='seg_ga02')(ga_02)\n",
    "    seg_ga03 = Conv2D(4, (1,1), name='seg_ga03')(ga_03)\n",
    "    #outputs for xent losses\n",
    "    output_xent = [seg_01, seg_02, seg_03, seg_ga01, seg_ga02, seg_ga03]\n",
    "    #output for dice coefficient loss\n",
    "    pred_seg = Add()(output_xent)\n",
    "    output_dice = Softmax()(pred_seg/len(output_xent))\n",
    "    return output_xent, output_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "input_layer = Input(shape=(240,240,4))\n",
    "model = Model(input_layer, forward(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_xent, output_dice = model(image, training=True)\n",
    "        loss_dice = generalized_dice_loss(label, output_dice)\n",
    "        loss_xents=[]\n",
    "        for seg in output_xent:\n",
    "            loss_xent = xent_logit(label, seg)\n",
    "            loss_xents.append(loss_xent)\n",
    "        loss_total = sum(loss_xents)+loss_dice\n",
    "    gradients = tape.gradient(loss_total, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return output_dice, loss_total, gradients\n",
    "\n",
    "@tf.function\n",
    "def val_fn(image, label):\n",
    "    model_output = model(image, training=False)\n",
    "    loss = custom_loss(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "#list\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "loss_inner = []\n",
    "while epochs <= max_epochs:\n",
    "    start = time.time()\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    ds = os.listdir(tfrecords_read_dir)\n",
    "    #shuffle directory list of tfrecords\n",
    "    shuffle = random.shuffle(ds)\n",
    "    for tf_re in ds:\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        acc_inner = []\n",
    "        for imgs in dataset:\n",
    "            #data augmentation\n",
    "            imgs = data_aug(imgs)\n",
    "            image = imgs[:,:,:,:4]\n",
    "            #unprocessed label for plotting\n",
    "            label = imgs[:,:,:,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            img_seg, loss, gradients = train_fn(image, label) #training function\n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32)\n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            #append accuracy for every steps\n",
    "            acc_inner.append(acc)\n",
    "            if epochs%5==0:\n",
    "                model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "                dc_list =compute_metric_dc(label,img_seg)\n",
    "                dc_app.append(dc_list)\n",
    "            #output\n",
    "            if steps%5000==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                loss_list.append(loss)\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp =compute_metric_dc(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Seq: TC, ET, WT\")\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "        acc_list.append(np.mean(acc_inner))\n",
    "    if epochs%5==0:\n",
    "        mean_dc = np.mean(np.array(dc_app),0)\n",
    "        print()\n",
    "        print('-----------<Summary for Epoch:{}>------------'.format(epochs))\n",
    "        print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "        #'core','enhancing','complete'\n",
    "        print(\"Seq: TC, ET, WT\")\n",
    "        print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "        print('------------------------------------------------')\n",
    "        print()\n",
    "    elapsed_time =(time.time()-start)/60 #unit in mins\n",
    "    print(\"Compute time per epochs: {:.2f} mins\".format(elapsed_time))\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "def output_fn(image):\n",
    "    model.trainable = False\n",
    "    model_output = model(image)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds = '/home/kevinteng/Desktop/ssd02/BraTS2020_preprocessed03/'\n",
    "# save_path = '/home/kevinteng/Desktop/ssd02/submission/'\n",
    "# actual_label = '/home/kevinteng/Desktop/ssd02/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii.gz'\n",
    "# #all brain affine are the same just pick one \n",
    "# brain_affine = nib.load(actual_label).affine\n",
    "# steps = 1\n",
    "# acc_list = []\n",
    "# for train_or_val in sorted(os.listdir(ds)):\n",
    "#     save_dir = save_path + train_or_val+'_'+ver\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     merge01 = os.path.join(ds+train_or_val)\n",
    "#     for patient in sorted(os.listdir(merge01)):\n",
    "#         patient_id = patient.split('.')[0]\n",
    "#         merge02 = os.path.join(merge01,patient)\n",
    "#         imgs = np.load(merge02)\n",
    "#         image = imgs[:,:,:,:4]\n",
    "#         seg_output = 0 #flush RAM\n",
    "#         seg_output = np.zeros((240,240,155))\n",
    "#         for i in range(image.shape[0]):\n",
    "#             inp = tf.expand_dims(image[i],0)\n",
    "#             img_seg = output_fn(inp) #validation function \n",
    "#             #map from sparse to label\n",
    "#             seg_output[:,:,i] = np.argmax(img_seg,-1) \n",
    "#         #convert label from 4 to 3 and np array and cast as int\n",
    "#         seg_output= np.where(seg_output==3,4,seg_output).astype(np.uint8)\n",
    "#         prediction_ni = nib.Nifti1Image(seg_output, brain_affine)\n",
    "#         prediction_ni.to_filename(save_dir+'/{}.nii.gz'.format(patient_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
