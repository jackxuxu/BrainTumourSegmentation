{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from utils import compute_metric_dc\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue => Label 1 (Necrotic and Non-enhancing Tumor Core)\n",
    "- Yellow => Label 2 (Peritumoral Edema)\n",
    "- Green => Label 3/4 (GD-Enhancing Tumor)\n",
    "---\n",
    "* Core => Label 1 & 3\n",
    "* Enhancing => Label 3\n",
    "* Complete => Label 1,2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "max_epochs = 20\n",
    "BATCH_SIZE = 8\n",
    "lr = 0.001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = 'model_self_attention_03' #save version\n",
    "dropout=0.2 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords05/'\n",
    "stack_npy = \"/home/kevinteng/Desktop/ssd02/BraTS2020_stack05/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def generalized_dice(y_true, y_pred, smooth = 1e-5):\n",
    "    \"\"\"\n",
    "    Generalized Dice Score\n",
    "    https://arxiv.org/pdf/1707.03237\n",
    "    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true    = tf.reshape(y_true,shape=(-1,4))\n",
    "    y_pred    = tf.reshape(y_pred,shape=(-1,4))\n",
    "    sum_p     = tf.reduce_sum(y_pred, -2)\n",
    "    sum_r     = tf.reduce_sum(y_true, -2)\n",
    "    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)\n",
    "    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)\n",
    "    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))\n",
    "    return generalized_dice\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred):   \n",
    "    return 1-generalized_dice(y_true, y_pred)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    The final loss function consists of the summation of two losses \"GDL\" and \"CE\"\n",
    "    with a regularization term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)\n",
    "\n",
    "def data_aug(imgs):\n",
    "    choice = np.random.randint(0,4)\n",
    "    #no augmentation \n",
    "    if choice==0:\n",
    "        x = imgs \n",
    "    #flip up and down \n",
    "    if choice==1:\n",
    "        x = tf.image.flip_up_down(imgs)\n",
    "    #flip left and right \n",
    "    if choice==2:\n",
    "        x = tf.image.flip_left_right(imgs)\n",
    "    #rotation based on angle \n",
    "    if choice==3:\n",
    "        n_rot = np.random.randint(1,4)\n",
    "        x = tf.image.rot90(imgs, k=n_rot)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Layer Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# template for guided attention block\n",
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Activation, Add, Multiply, GaussianNoise\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Layer, Softmax, ReLU, PReLU\n",
    "from tensorflow_addons.layers import GroupNormalization\n",
    "from utils_model import *\n",
    "from attention import *\n",
    "\n",
    "def conv_block_sep_v2(x, filters, layer_name, norm_fn='bn', kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti_fn='relu', dropout_rate=None):\n",
    "    '''\n",
    "    Dual convolution block with [full pre-activation], Norm -> Acti -> Conv\n",
    "    :param x: Input features\n",
    "    :param filters: A list that contains the number of filters for 1st and 2nd convolutional layer\n",
    "    :param layer_name: A list  that contains the name for the 1st and 2nd convolutional layer\n",
    "    :param norm_fn: Tensorflow function for normalization, 'bn' for Batch Norm, 'gn' for Group Norm\n",
    "    :param kernel_size: Kernel size for both convolutional layer with 3x3 as default\n",
    "    :param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default\n",
    "    :param acti_fn: Tensorflow function for activation, 'relu' for ReLU, 'prelu' for PReLU\n",
    "    :param dropout_rate: Specify dropouts for layers\n",
    "    :return: Feature maps of same size as input with number of filters equivalent to the last layer\n",
    "    '''\n",
    "    assert type(filters)==list, \"Please input filters of type list.\"\n",
    "    assert type(layer_name)==list, \"Please input filters of type list.\"\n",
    "    assert acti_fn!= None, 'There should be an activation functino specified'\n",
    "    #1st convolutional block\n",
    "    if norm_fn=='bn':\n",
    "        x = BatchNormalization()(x)\n",
    "    elif norm_fn=='gn':\n",
    "        x = GroupNormalization()(x)\n",
    "    if acti_fn=='relu':\n",
    "        x = ReLU()(x)\n",
    "    elif acti_fn=='prelu':\n",
    "        x = PReLU(shared_axes=[1,2])(x)\n",
    "    x = SeparableConv2D(filters[0], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[0])(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    #2nd convolutional block\n",
    "    if norm_fn=='bn':\n",
    "        x = BatchNormalization()(x)\n",
    "    elif norm_fn=='gn':\n",
    "        x = GroupNormalization()(x)\n",
    "    if acti_fn=='relu':\n",
    "        x = ReLU()(x)\n",
    "    elif acti_fn=='prelu':\n",
    "        x = PReLU(shared_axes=[1,2])(x)\n",
    "    x = SeparableConv2D(filters[1], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[1])(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def down_sampling_sep_v2(x, filters, layer_name, norm_fn='bn', kernel_size=(3, 3), acti_fn='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None, mode ='coord', x_dim=None, y_dim=None):\n",
    "    '''\n",
    "    Down sampling function version 2 with Convolutional layer of stride 2 as downsampling operation, with\n",
    "    [full pre-activation], Norm -> Acti -> Conv\n",
    "    :param x: Input features\n",
    "    :param filters: Number of filters for Convolutional layer of stride 2\n",
    "    :param layer_name: Layer name for convolutional layer\n",
    "    :param norm_fn: Tensorflow function for normalization, 'bn' for Batch Norm, 'gn' for Group Norm\n",
    "    :param kernel_size: Kernel size for both convolutional layer with 3x3 as default\n",
    "    :param acti_fn: Tensorflow function for activation, 'relu' for ReLU, 'prelu' for PReLU\n",
    "    :param kernel_initializer: Initializer for kernel weights with 'glorot uniform' as default\n",
    "    :param dropout_rate: Specify dropouts for layers\n",
    "    :param mode: 'coord' for Seperable Coord Conv, 'normal' for Seperable Conv\n",
    "    :param x_dim: x dimension for coord conv\n",
    "    :param y_dim: y dimension for coord conv\n",
    "    :return: Feature maps of size scaled down by 2 with number of filters specified\n",
    "    '''\n",
    "    assert mode=='coord' or mode=='normal',  \"Use 'coord' or 'normal' for mode!\"\n",
    "    assert acti_fn!= None, 'There should be an activation functino specified'\n",
    "    if norm_fn=='bn':\n",
    "        x = BatchNormalization()(x)\n",
    "    elif norm_fn=='gn':\n",
    "        x = GroupNormalization()(x)\n",
    "    if acti_fn=='relu':\n",
    "        x = ReLU()(x)\n",
    "    elif acti_fn=='prelu':\n",
    "        x = PReLU(shared_axes=[1,2])(x)\n",
    "    if mode=='coord':\n",
    "        #seperable coordconv\n",
    "        assert (x_dim!=None and y_dim!=None), \"Please input dimension for CoordConv!\"\n",
    "        x = Conv2D(1, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer)(x)\n",
    "        x = CoordConv(x_dim=x_dim, y_dim=y_dim, with_r=False, filters=filters, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x)\n",
    "    else:\n",
    "        #normal mode\n",
    "        x = SeparableConv2D(filters, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def res_block_sep_v2(x_in, filters,  layer_name, norm_fn='gn', kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti_fn='prelu', dropout_rate=None):\n",
    "    assert len(filters)==2, \"Please assure that there is 3 values for filters.\"\n",
    "    assert len(layer_name)==3, \"Please assure that there is 3 values for layer name\"\n",
    "    layer_name_conv = [layer_name[i] for i in range(len(layer_name)-1)]\n",
    "    output_conv_block = conv_block_sep_v2(x_in, filters, layer_name_conv, norm_fn=norm_fn, kernel_size=kernel_size,\n",
    "                                   kernel_initializer = kernel_initializer, acti_fn = acti_fn, dropout_rate=dropout_rate)\n",
    "    output_add = Add(name = layer_name[-1])([output_conv_block, x_in])\n",
    "    return output_add\n",
    "\n",
    "def guided_attention_block(inp_feature, layer_name_p, layer_name_c):\n",
    "    '''\n",
    "    Guided attention block that takes feature as input and concatenates features\n",
    "    from PAM and CAM as output\n",
    "    :param inp_feature: Input features\n",
    "    :param layer_name_p: layer name list for PAM\n",
    "    :param layer_name_c: layer name list for CAM\n",
    "    :return: squeezed concatenated features of PAM and CAM\n",
    "    '''\n",
    "    pam_feature = PAM(inp_feature, layer_name_p, kernel_initializer=hn)\n",
    "    cam_feature = CAM(inp_feature, layer_name_c)\n",
    "    add = Add()([pam_feature,cam_feature]) #[60,60,128]\n",
    "    up = UpSampling2D(size=(4,4))(add) #[240,240,128]\n",
    "    squeeze = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn,\n",
    "                       activation='relu')(up)\n",
    "    return squeeze\n",
    "\n",
    "def guided_attention(res_feature, ms_feature, layer_name):\n",
    "    '''\n",
    "    Guided attention module\n",
    "    :param res_feature: Upsampled Feature maps from Res Block\n",
    "    :param ms_feature: Multi scale feature maps result from Res Block\n",
    "    :param layer_name: Layer Name should consist be a list contating 4 list\n",
    "    Example:\n",
    "    layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                      'pam01_alpha','pam01_add']\n",
    "    layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "    layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                      'pam02_alpha', 'pam02_add']\n",
    "    layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "    layer_name = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "    :return: guided attention module with shape same as input\n",
    "    '''\n",
    "    assert len(layer_name)==4, \"Layer name should be a list consisting 4 lists!\"\n",
    "    #self attention block01\n",
    "    concat01 = concatenate([res_feature, ms_feature], axis=-1)\n",
    "    squeeze01 = guided_attention_block(concat01, layer_name[0], layer_name[1])\n",
    "    multi01 = Multiply()([squeeze01, ms_feature])\n",
    "    #self attention block02\n",
    "#     concat02 = concatenate([multi01, res_feature],axis=-1)\n",
    "#     squeeze02 = guided_attention_block(concat02, layer_name[2], layer_name[3])\n",
    "    return multi01\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Resnet as backbone for multiscale feature retrieval.\n",
    "    Each resblock output(input signal), next resblock output(gated signal) is\n",
    "    feed into the gated attention for multi scale feature refinement.\n",
    "    Each gated attention output is pass through a bottle neck layer to standardize\n",
    "    the channel size by squashing them to desired filter size of 64.\n",
    "    The features are upsampled at each block to the corresponding [wxh] dimension\n",
    "    of w:240, h:240.\n",
    "    The upsampled features are concat and squash to corresponding channel size of 64\n",
    "    which yield multiscale feature.\n",
    "    :param x: batched images\n",
    "    :return: feature maps of each res block\n",
    "    '''\n",
    "    #inject noise\n",
    "    gauss1 = GaussianNoise(0.01)(x)\n",
    "    #retrieve input dimension\n",
    "    b,w,h,c = x.shape\n",
    "    #---- ResNet and Multiscale Features----\n",
    "    #1st block\n",
    "    conv01 = CoordConv(x_dim=w, y_dim=h, with_r=False, filters=64, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=hn, name='conv01')(gauss1)\n",
    "    res_block01 = res_block_sep_v2(conv01, filters=[128, 64], layer_name=[\"conv02\", \"conv03\", \"add01\"], dropout_rate=dropout)\n",
    "    #2nd block\n",
    "    down_01 = down_sampling_sep_v2(res_block01, filters=128, layer_name = 'down_01',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=w//2, y_dim=w//2)\n",
    "    res_block02 = res_block_sep_v2(down_01, filters=[256, 128], layer_name=[\"conv04\", \"conv05\", \"add02\"], dropout_rate=dropout)\n",
    "    #3rd block\n",
    "    down_02 = down_sampling_sep_v2(res_block02, filters=256, layer_name = 'down_02',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=w//4, y_dim=h//4)\n",
    "    res_block03 = res_block_sep_v2(down_02, filters=[512, 256], layer_name=[\"conv06\", \"conv07\", \"add03\"], dropout_rate=dropout)\n",
    "    #4th block\n",
    "    down_03 = down_sampling_sep_v2(res_block03, filters=512, layer_name = 'down_03',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=w//8, y_dim=h//8)\n",
    "    res_block04 = res_block_sep_v2(down_03, filters=[1024, 512], layer_name=[\"conv08\", \"conv09\", \"add04\"], dropout_rate=dropout)\n",
    "    # *apply activation function for the last output\n",
    "    res_block04 = PReLU(shared_axes=[1,2])(res_block04)\n",
    "    #grid attention blocks\n",
    "    att_block01 = attention_block(res_block01,res_block02,64,'grid_att01')\n",
    "    att_block02 = attention_block(res_block02,res_block03,128,'grid_att02')\n",
    "    att_block03 = attention_block(res_block03, res_block04,256,'gird_att03')\n",
    "    #bottle neck => layer squash all attention block to same filter size 64\n",
    "    bottle01 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block01)\n",
    "    bottle02 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block02)\n",
    "    bottle03 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block03)\n",
    "    #upsampling for all layers to same (wxh) dimension=>240x240\n",
    "    up01 = bottle01 #[240,240,64]\n",
    "    up02 = UpSampling2D(size=(2, 2), interpolation='bilinear')(bottle02) #[120,120,64]=>[240,240,64]\n",
    "    up03 = UpSampling2D(size=(4,4), interpolation='bilinear')(bottle03) #[60,60,64]=>[240,240,64]\n",
    "    #multiscale features\n",
    "    concat_all = concatenate([up01,up02,up03],axis=-1) #[240,240,3*64]\n",
    "    #squeeze to have the same channel as upsampled features [240,240,3*64] => [240,240,64]\n",
    "    ms_feature = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(concat_all)\n",
    "    #Segmentations from multiscale features *without softmax activation\n",
    "    seg_01 = Conv2D(4, (1,1), name='seg_01')(up01)\n",
    "    seg_02 = Conv2D(4, (1,1), name='seg_02')(up02)\n",
    "    seg_03 = Conv2D(4, (1,1), name='seg_03')(up02)\n",
    "\n",
    "    #----self guided attention blocks-----\n",
    "    ga_01 = guided_attention(up01, ms_feature, layer_name_ga[0])\n",
    "    ga_02 = guided_attention(up02, ms_feature, layer_name_ga[1])\n",
    "    ga_03 = guided_attention(up03, ms_feature, layer_name_ga[2])\n",
    "    #Segmentations from guided attention features *without softmax activation\n",
    "    seg_ga01 = Conv2D(4, (1,1), name='seg_ga01')(ga_01)\n",
    "    seg_ga02 = Conv2D(4, (1,1), name='seg_ga02')(ga_02)\n",
    "    seg_ga03 = Conv2D(4, (1,1), name='seg_ga03')(ga_03)\n",
    "    #outputs for xent losses\n",
    "    output_xent = [seg_01, seg_02, seg_03, seg_ga01, seg_ga02, seg_ga03]\n",
    "    #output for dice coefficient loss\n",
    "    pred_seg = Add()(output_xent)\n",
    "    output_dice = Softmax()(pred_seg/len(output_xent))\n",
    "    return output_xent, output_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "input_layer = Input(shape=(200,200,4))\n",
    "model = Model(input_layer, forward(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_xent, output_dice = model(image, training=True)\n",
    "        loss_dice = generalized_dice_loss(label, output_dice)\n",
    "        loss_xents=[]\n",
    "        for seg in output_xent:\n",
    "            loss_xent = xent_logit(label, seg)\n",
    "            loss_xents.append(loss_xent)\n",
    "        loss_total = sum(loss_xents)+loss_dice\n",
    "    gradients = tape.gradient(loss_total, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return output_dice, loss_total, gradients\n",
    "\n",
    "@tf.function\n",
    "def val_fn(image, label):\n",
    "    model_output = model(image, training=False)\n",
    "    loss = custom_loss(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "#list\n",
    "loss_list = []\n",
    "start_runtime = time.time()\n",
    "while epochs <= max_epochs:\n",
    "    start = time.time()\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    ds = os.listdir(tfrecords_read_dir)\n",
    "    #shuffle directory list of tfrecords\n",
    "    shuffle = random.shuffle(ds)\n",
    "    loss_inner = []\n",
    "    for tf_re in ds:\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        for imgs in dataset:\n",
    "            #data augmentation\n",
    "            imgs = data_aug(imgs)\n",
    "            #crop images\n",
    "            image = imgs[:,20:220,20:220,:4]\n",
    "            #unprocessed label for plotting (cropped)\n",
    "            label = imgs[:,20:220,20:220,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            #--------------<training function>----------------------------\n",
    "            img_seg, loss, gradients = train_fn(image, label) \n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32)\n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            #store lost for every steps\n",
    "            loss_inner.append(loss)\n",
    "            #save weights for every 5 epochs \n",
    "            if epochs%5==0:\n",
    "                model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "            #output\n",
    "            if steps%5000==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp =compute_metric_dc(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Seq: TC, ET, WT\")\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "    loss_list.append(np.mean(loss_inner))\n",
    "    #end time per epochs \n",
    "    elapsed_time =(time.time()-start)/60 #unit in mins\n",
    "    print(\"Compute time per epochs: {:.2f} mins\".format(elapsed_time))\n",
    "    epochs+=1\n",
    "#end time for total epochs\n",
    "elapsed_time_runtime = (time.time()-start_runtime)/60\n",
    "print()\n",
    "print('----------------------------------<END>---------------------------------')\n",
    "print(\"Total run time for {} epochs: {:.2f} mins\".format(epochs, elapsed_time_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "def output_fn(image):\n",
    "    w,h,c = image.shape\n",
    "    model.trainable = False\n",
    "    _, model_output = model(image)\n",
    "    # we need [240,240,155] to input into cloud validation\n",
    "    if w!=240:\n",
    "        #padding constant\n",
    "        p = int(240-w)\n",
    "        padding = tf.constant([[0,0],[p,p],[p,p],[0,0]]) #p=20\n",
    "        model_output = tf.pad(model_output, padding, \"CONSTANT\")\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = '/home/kevinteng/Desktop/ssd02/BraTS2020_preprocessed05/'\n",
    "save_path = '/home/kevinteng/Desktop/ssd02/submission/'\n",
    "actual_label = '/home/kevinteng/Desktop/ssd02/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii.gz'\n",
    "#all brain affine are the same just pick one \n",
    "brain_affine = nib.load(actual_label).affine\n",
    "for train_or_val in sorted(os.listdir(ds)):\n",
    "    save_dir = save_path + train_or_val+'_'+ver\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    merge01 = os.path.join(ds+train_or_val)\n",
    "    for patient in sorted(os.listdir(merge01)):\n",
    "        patient_id = patient.split('.')[0]\n",
    "        merge02 = os.path.join(merge01,patient)\n",
    "        imgs = np.load(merge02)\n",
    "        image = imgs[:,20:220,20:220,:4]\n",
    "        seg_output = 0 #flush RAM\n",
    "        seg_output = np.zeros((240,240,155))\n",
    "        for i in range(image.shape[0]):\n",
    "            inp = tf.expand_dims(image[i],0)\n",
    "            img_seg = output_fn(inp) #validation function \n",
    "            #map from sparse to label\n",
    "            seg_output[:,:,i] = np.argmax(img_seg,-1) \n",
    "        #convert label from 4 to 3 and np array and cast as int\n",
    "        seg_output= np.where(seg_output==3,4,seg_output).astype(np.uint8)\n",
    "        prediction_ni = nib.Nifti1Image(seg_output, brain_affine)\n",
    "        prediction_ni.to_filename(save_dir+'/{}.nii.gz'.format(patient_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
