{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from utils import dice_coef, ss_metric, compute_metric\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue => Label 1 (Necrotic and Non-enhancing Tumor Core)\n",
    "- Yellow => Label 2 (Peritumoral Edema)\n",
    "- Green => Label 3/4 (GD-Enhancing Tumor)\n",
    "---\n",
    "* Core => Label 1 & 3\n",
    "* Enhancing => Label 3\n",
    "* Complete => Label 1,2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "max_epochs = 2\n",
    "BATCH_SIZE = 24\n",
    "lr = 0.000001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = 'self_attention_01' #save version \n",
    "dropout=0.3 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/'\n",
    "stack_npy = \"/home/kevinteng/Desktop/ssd02/BraTS2020_stack03/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def generalized_dice(y_true, y_pred, smooth = 1e-5):\n",
    "    \"\"\"\n",
    "    Generalized Dice Score\n",
    "    https://arxiv.org/pdf/1707.03237\n",
    "    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true    = tf.reshape(y_true,shape=(-1,4))\n",
    "    y_pred    = tf.reshape(y_pred,shape=(-1,4))\n",
    "    sum_p     = tf.reduce_sum(y_pred, -2)\n",
    "    sum_r     = tf.reduce_sum(y_true, -2)\n",
    "    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)\n",
    "    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)\n",
    "    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))\n",
    "    return generalized_dice\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred):   \n",
    "    return 1-generalized_dice(y_true, y_pred)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    The final loss function consists of the summation of two losses \"GDL\" and \"CE\"\n",
    "    with a regularization term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)\n",
    "\n",
    "def data_aug(imgs, seed=8888):\n",
    "    x = tf.image.random_flip_up_down(imgs,seed)\n",
    "    x = tf.image.random_flip_left_right(x,seed)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Layer Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# template for guided attention block\n",
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import conv_block, coordconv_block, up, pool, attention_block\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Activation, Add, Multiply, GaussianNoise\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, Dropout, Softmax, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Layer, Softmax\n",
    "from coord_conv import CoordConv\n",
    "\n",
    "def conv_block_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert type(filters)==list, \"Please input filters of type list.\"\n",
    "    assert type(layer_name)==list, \"Please input filters of type list.\"\n",
    "    x = SeparableConv2D(filters[0], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[0])(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    x = SeparableConv2D(filters[1], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[1])(x)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = SeparableConv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def down_sampling_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None, mode ='coord', x_dim=None, y_dim=None):\n",
    "    assert mode=='coord' or mode=='normal', \"Use 'coord' or 'normal' for mode!\"\n",
    "    if mode=='coord':\n",
    "        #seperable coordconv\n",
    "        assert (x_dim!=None and y_dim!=None), \"Please input dimension for CoordConv!\"\n",
    "        x = Conv2D(1, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer)(x_in)\n",
    "        x = CoordConv(x_dim=x_dim, y_dim=y_dim, with_r=False, filters=filters, strides=(1,1), \n",
    "                      kernel_size = 3, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x)\n",
    "    else:\n",
    "        #normal mode\n",
    "        x = SeparableConv2D(filters, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def res_block_sep(x_in, filters,  layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert len(filters)==2, \"Please assure that there is 3 values for filters.\"\n",
    "    assert len(layer_name)==3, \"Please assure that there is 3 values for layer name\"\n",
    "    layer_name_conv = [layer_name[i] for i in range(len(layer_name)-1)]\n",
    "    output_conv_block = conv_block_sep(x_in, filters, layer_name_conv, batch_norm=batch_norm, kernel_size=kernel_size,\n",
    "                                   kernel_initializer = kernel_initializer, acti = acti, dropout_rate=dropout_rate)\n",
    "    output_add = Add(name = layer_name[-1])([output_conv_block, x_in])\n",
    "    return output_add\n",
    "\n",
    "def attention_block(input_signal, gated_signal, filters, att_layer_name):\n",
    "    #input signal feature maps\n",
    "    is_fm = Conv2D(filters, kernel_size=(1,1), strides=(2, 2), padding = 'same')(input_signal)\n",
    "    #gated signal feature maps\n",
    "    gs_fm = Conv2D(filters, kernel_size=(1,1), strides=(1, 1), padding = 'same')(gated_signal)\n",
    "    #debugger\n",
    "    assert is_fm.shape!=gs_fm.shape, \"Feature maps shape doesn't match!\"\n",
    "    #element wise sum\n",
    "    add = Add()([is_fm, gs_fm])\n",
    "    acti = Activation('relu')(add)\n",
    "    #downsampled attention coefficient\n",
    "    bottle_neck = Conv2D(1, kernel_size=(1,1), activation='sigmoid', name=att_layer_name)(acti)\n",
    "    #bilinear interpolation to get attention coeffcient\n",
    "    alpha = UpSampling2D(interpolation='bilinear')(bottle_neck)\n",
    "    #filter off input signal's features with attention coefficient\n",
    "    multi = Multiply()([input_signal, alpha])\n",
    "    return multi\n",
    "\n",
    "class att_var(Layer):\n",
    "    '''\n",
    "    Attention variable\n",
    "    '''\n",
    "    def __init__(self, initial_val):\n",
    "        super(att_var, self).__init__()\n",
    "        self.initial_val = initial_val\n",
    "    def __call__(self):\n",
    "        alpha = tf.Variable(initial_value=self.initial_val, trainable=True)\n",
    "        return alpha\n",
    "\n",
    "def PAM(inp_feature, layer_name, kernel_initializer='glorot_uniform', acti='relu'):\n",
    "    '''\n",
    "    Position attention module\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param layer_name: List of layer names\n",
    "    [1st conv block, 2nd conv block, softmax output, 3rd conv block, position coefficient, Add output]\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]\n",
    "    :return: PAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #scale down ratio\n",
    "    c_8 = c//8\n",
    "    #\n",
    "    assert len(layer_name)>=5, 'Layer list length should be 5!'\n",
    "    # Branch01 Dimension: [w,h,c/8] => [(wxh),c/8]\n",
    "    query = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[0], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "            kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    query = tf.reshape(query,[-1,(w*h),c_8 ])\n",
    "    # Branch02 Dimension: [w,h,c/8] => [c/8,(wxh)]\n",
    "    key = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[1], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    key = tf.reshape(key, [-1,(w*h),c_8 ])\n",
    "    key = tf.einsum('bij->bji', key) #transpose/permutation\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0102 = tf.einsum('bij,bjk->bik', query, key) #[(wxh),(wxh)]\n",
    "    softmax0102 = Softmax(name=layer_name[2])(matmul_0102)\n",
    "    # Branch03\n",
    "    value = conv_2d(inp_feature, filters=c, layer_name=layer_name[3], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    value = tf.reshape(value,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('bij,bjk->bik',softmax0102,value) #[(wxh),c]\n",
    "    # Output\n",
    "    output = tf.reshape(matmul_all, [-1,w,h,c]) #[w,h,c]\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    lambda_p = Conv2D(filters=1,kernel_size=1,padding='same',activation='sigmoid', name=layer_name[4])(inp_feature)\n",
    "    output = Multiply()([output, lambda_p])\n",
    "    output_add = Add(name = layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def CAM(inp_feature, layer_name):\n",
    "    '''\n",
    "    Channel attention\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]k\n",
    "    :param layer_name: List of layer names\n",
    "        [softmax output, channel attention coefficients, Add output]\n",
    "    :return: CAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    assert len(layer_name)>=2, 'Layer list length should be 2!'\n",
    "    # Branch01 Dimension: [w,h,c] => [(wxh),c]\n",
    "    query = tf.reshape(inp_feature, [-1,(w*h),c])\n",
    "    # Branch02 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    key = tf.reshape(inp_feature, [-1,(w*h),c]) #[(wxh),c]\n",
    "    key = tf.einsum('ijk->ikj', key) #Permute:[c,(wxh)]\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0201 = tf.einsum('ijk,ikl->ijl', key, query) #[c,c]\n",
    "    softmax0102 = Softmax(name=layer_name[0])(matmul_0201)\n",
    "    # Branch03 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    value = tf.reshape(inp_feature,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('ijk,ikl->ijl', value,softmax0102) #[(wxh),c]\n",
    "    #output\n",
    "    output = tf.reshape(matmul_all,[-1,w,h,c])#[w,h,c]\n",
    "    #provides learnable parameter\n",
    "    #*channel wise attention, inspired by Squeeze Excitation(SE) block\n",
    "    GAP = GlobalAveragePooling2D()(output)\n",
    "    dense01 = Dense(c//8, activation='relu')(GAP)\n",
    "    alpha_c = Dense(c,activation='sigmoid', name=layer_name[1])(dense01)\n",
    "    #outputs\n",
    "    output = Multiply()([output, alpha_c])\n",
    "    output_add = Add(name=layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def guided_attention_block(inp_feature, layer_name_p, layer_name_c):\n",
    "    '''\n",
    "    Guided attention block that takes feature as input and concatenates features\n",
    "    from PAM and CAM as output\n",
    "    :param inp_feature: Input features\n",
    "    :param layer_name_p: layer name list for PAM\n",
    "    :param layer_name_c: layer name list for CAM\n",
    "    :return: squeezed concatenated features of PAM and CAM\n",
    "    '''\n",
    "    pam_feature = PAM(inp_feature, layer_name_p, kernel_initializer=hn)\n",
    "    cam_feature = CAM(inp_feature, layer_name_c)\n",
    "    add = Add()([pam_feature,cam_feature])\n",
    "    squeeze = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn,\n",
    "                       activation='relu')(add)\n",
    "    return squeeze\n",
    "\n",
    "def guided_attention(res_feature, ms_feature, layer_name):\n",
    "    '''\n",
    "    Guided attention module\n",
    "    :param res_feature: Upsampled Feature maps from Res Block\n",
    "    :param ms_feature: Multi scale feature maps result from Res Block\n",
    "    :param layer_name: Layer Name should consist be a list contating 4 list\n",
    "    Example:\n",
    "    layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                      'pam01_alpha','pam01_add']\n",
    "    layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "    layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                      'pam02_alpha', 'pam02_add']\n",
    "    layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "    layer_name = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "    :return: guided attention module with shape same as input\n",
    "    '''\n",
    "    assert len(layer_name)==4, \"Layer name should be a list consisting 4 lists!\"\n",
    "    #self attention block01\n",
    "    concat01 = concatenate([res_feature, ms_feature], axis=-1)\n",
    "    squeeze01 = guided_attention_block(concat01, layer_name[0], layer_name[1])\n",
    "    multi01 = Multiply()([squeeze01, ms_feature])\n",
    "    #self attention block02\n",
    "    concat02 = concatenate([multi01, res_feature])\n",
    "    squeeze02 = guided_attention_block(concat02, layer_name[2], layer_name[3])\n",
    "    return squeeze02\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Resnet as backbone for multiscale feature retrieval.\n",
    "    Each resblock output(input signal), next resblock output(gated signal) is\n",
    "    feed into the gated attention for multi scale feature refinement.\n",
    "    Each gated attention output is pass through a bottle neck layer to standardize\n",
    "    the channel size by squashing them to desired filter size of 64.\n",
    "    The features are upsampled at each block to the corresponding [wxh] dimension\n",
    "    of w:240, h:240.\n",
    "    The upsampled features are concat and squash to corresponding channel size of 64\n",
    "    which yield multiscale feature.\n",
    "    :param x: batched images\n",
    "    :return: feature maps of each res block\n",
    "    '''\n",
    "    #inject noise\n",
    "    gauss1 = GaussianNoise(0.01)(x)\n",
    "    #---- ResNet and Multiscale Features----\n",
    "    #1st block\n",
    "    conv01 = CoordConv(x_dim=240, y_dim=240, with_r=False, filters=64, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=hn, name='conv01')(gauss1)\n",
    "    res_block01 = res_block_sep(conv01, filters=[128, 64], layer_name=[\"conv02\", \"conv03\", \"add01\"])\n",
    "    #2nd block\n",
    "    down_01 = down_sampling_sep(res_block01, filters=128, layer_name = 'down_01',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=120, y_dim=120)\n",
    "    res_block02 = res_block_sep(down_01, filters=[256, 128], layer_name=[\"conv04\", \"conv05\", \"add02\"])\n",
    "    #3rd block\n",
    "    down_02 = down_sampling_sep(res_block02, filters=256, layer_name = 'down_02',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=60, y_dim=60)\n",
    "    res_block03 = res_block_sep(down_02, filters=[512, 256], layer_name=[\"conv06\", \"conv07\", \"add03\"])\n",
    "    #4th block\n",
    "    down_03 = down_sampling_sep(res_block03, filters=512, layer_name = 'down_03',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=30, y_dim=30)\n",
    "    res_block04 = res_block_sep(down_03, filters=[1024, 512], layer_name=[\"conv08\", \"conv09\", \"add04\"])\n",
    "    #grid attention blocks\n",
    "    att_block01 = attention_block(res_block01,res_block02,64,'grid_att01')\n",
    "    att_block02 = attention_block(res_block02,res_block03,128,'grid_att02')\n",
    "    att_block03 = attention_block(res_block03, res_block04,256,'gird_att03')\n",
    "    #bottle neck => layer squash all attention block to same filter size 64\n",
    "    bottle01 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block01)\n",
    "    bottle02 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block02)\n",
    "    bottle03 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block03)\n",
    "    #upsampling for all layers to same (wxh) dimension=>240x240\n",
    "    up01 = bottle01 #[240,240,64]\n",
    "    up02 = UpSampling2D(size=(2, 2), interpolation='bilinear')(bottle02) #[120,120,64]=>[240,240,64]\n",
    "    up03 = UpSampling2D(size=(4,4), interpolation='bilinear')(bottle03) #[60,60,64]=>[240,240,64]\n",
    "    #multiscale features\n",
    "    concat_all = concatenate([up01,up02,up03],axis=-1) #[240,240,3*64]\n",
    "    #squeeze to have the same channel as upsampled features [240,240,3*64] => [240,240,64]\n",
    "    ms_feature = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(concat_all)\n",
    "    #Segmentations from multiscale features *without softmax activation\n",
    "    seg_01 = Conv2D(4, (1,1), name='seg_01')(up01)\n",
    "    seg_02 = Conv2D(4, (1,1), name='seg_02')(up02)\n",
    "    seg_03 = Conv2D(4, (1,1), name='seg_03')(up02)\n",
    "\n",
    "    #----self guided attention blocks-----\n",
    "    ga_01 = guided_attention(up01, ms_feature, layer_name_ga[0])\n",
    "    ga_02 = guided_attention(up02, ms_feature, layer_name_ga[1])\n",
    "    ga_03 = guided_attention(up03, ms_feature, layer_name_ga[2])\n",
    "    #Segmentations from guided attention features *without softmax activation\n",
    "    seg_ga01 = Conv2D(4, (1,1), name='seg_ga01')(ga_01)\n",
    "    seg_ga02 = Conv2D(4, (1,1), name='seg_ga02')(ga_02)\n",
    "    seg_ga03 = Conv2D(4, (1,1), name='seg_ga03')(ga_03)\n",
    "    #outputs for xent losses\n",
    "    output_xent = [seg_01, seg_02, seg_03, seg_ga01, seg_ga02, seg_ga03]\n",
    "    #output for dice coefficient loss\n",
    "    pred_seg = Add()(output_xent)\n",
    "    pred_seg = pred_seg/len(output_xent)\n",
    "    output_dice = Softmax()(pred_seg/len(output_xent))\n",
    "    return output_xent, output_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "input_layer = Input(shape=(240,240,4))\n",
    "model = Model(input_layer, forward(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_xent, output_dice = model(image, training=True)\n",
    "        loss_dice = generalized_dice_loss(label, output_dice)\n",
    "        loss_xents=[]\n",
    "        for seg in output_xent:\n",
    "            loss_xent = xent_logit(label, seg)\n",
    "            loss_xents.append(loss_xent)\n",
    "        loss_total = sum(loss_xents)+loss_dice\n",
    "    gradients = tape.gradient(loss_total, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return output_dice, loss_total, gradients\n",
    "\n",
    "@tf.function\n",
    "def val_fn(image, label):\n",
    "    model_output = model(image, training=False)\n",
    "    loss = custom_loss(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-70abeddcc812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHUFFLE_BUFFER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0macc_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m#data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2_1/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2607\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2608\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "#list\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "loss_inner = []\n",
    "while epochs <= max_epochs:\n",
    "    start = time.time()\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    ds = os.listdir(tfrecords_read_dir)\n",
    "    #shuffle directory list of tfrecords\n",
    "    shuffle = random.shuffle(ds)\n",
    "    for tf_re in ds:\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        acc_inner = []\n",
    "        for imgs in dataset:\n",
    "            #data augmentation\n",
    "            imgs = data_aug(imgs)\n",
    "            image = imgs[:,:,:,:4]\n",
    "            #unprocessed label for plotting\n",
    "            label = imgs[:,:,:,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            img_seg, loss, gradients = train_fn(image, label) #training function\n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32)\n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            #append accuracy for every steps\n",
    "            acc_inner.append(acc)\n",
    "            if epochs%5==0:\n",
    "                dc_list, sens_list, spec_list =compute_metric(label,img_seg)\n",
    "                dc_app.append(dc_list)\n",
    "                sens_app.append(sens_list)\n",
    "                spec_app.append(spec_list)\n",
    "            #output\n",
    "            if steps%1==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                loss_list.append(loss)\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp, sens_list_stp, spec_list_stp =compute_metric(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Sensitivity: {}\".format(sens_list_stp))\n",
    "                print(\"Specificity: {}\".format(spec_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "        acc_list.append(np.mean(acc_inner))\n",
    "    if epochs%5==0:\n",
    "        mean_dc = np.mean(np.array(dc_app),0)\n",
    "        mean_sens = np.mean(np.array(sens_app),0)\n",
    "        mean_spec = np.mean(np.array(spec_app),0)\n",
    "        print()\n",
    "        print('-----------<Summary for Epoch:{}>------------'.format(epochs))\n",
    "        print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "        #'core','enhancing','complete'\n",
    "        print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "        print(\"Mean Sensitivity: {}\".format(mean_sens))\n",
    "        print(\"Mean Specificity: {}\".format(mean_spec))\n",
    "        print('------------------------------------------------')\n",
    "        print()\n",
    "    elapsed_time =(time.time()-start)/60 #unit in mins\n",
    "    print(\"Compute time per epochs: {:.2f} mins\".format(elapsed_time))\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "def output_fn(image):\n",
    "    model.trainable = False\n",
    "    model_output = model(image)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds = '/home/kevinteng/Desktop/ssd02/BraTS2020_preprocessed03/'\n",
    "# save_path = '/home/kevinteng/Desktop/ssd02/submission/'\n",
    "# actual_label = '/home/kevinteng/Desktop/ssd02/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii.gz'\n",
    "# #all brain affine are the same just pick one \n",
    "# brain_affine = nib.load(actual_label).affine\n",
    "# steps = 1\n",
    "# acc_list = []\n",
    "# for train_or_val in sorted(os.listdir(ds)):\n",
    "#     save_dir = save_path + train_or_val+'_'+ver\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     merge01 = os.path.join(ds+train_or_val)\n",
    "#     for patient in sorted(os.listdir(merge01)):\n",
    "#         patient_id = patient.split('.')[0]\n",
    "#         merge02 = os.path.join(merge01,patient)\n",
    "#         imgs = np.load(merge02)\n",
    "#         image = imgs[:,:,:,:4]\n",
    "#         seg_output = 0 #flush RAM\n",
    "#         seg_output = np.zeros((240,240,155))\n",
    "#         for i in range(image.shape[0]):\n",
    "#             inp = tf.expand_dims(image[i],0)\n",
    "#             img_seg = output_fn(inp) #validation function \n",
    "#             #map from sparse to label\n",
    "#             seg_output[:,:,i] = np.argmax(img_seg,-1) \n",
    "#         #convert label from 4 to 3 and np array and cast as int\n",
    "#         seg_output= np.where(seg_output==3,4,seg_output).astype(np.uint8)\n",
    "#         prediction_ni = nib.Nifti1Image(seg_output, brain_affine)\n",
    "#         prediction_ni.to_filename(save_dir+'/{}.nii.gz'.format(patient_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 240, 240, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 240, 240, 4)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "coord_conv (CoordConv)          (None, 240, 240, 64) 3520        gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv02 (SeparableConv2D)        (None, 240, 240, 128 8896        coord_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 240, 240, 128 0           conv02[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv03 (SeparableConv2D)        (None, 240, 240, 64) 9408        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 240, 240, 64) 0           conv03[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add01 (Add)                     (None, 240, 240, 64) 0           activation_1[0][0]               \n",
      "                                                                 coord_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "down_01 (SeparableConv2D)       (None, 120, 120, 128 8896        add01[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 120, 120, 128 0           down_01[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv04 (SeparableConv2D)        (None, 120, 120, 256 34176       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 120, 120, 256 0           conv04[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv05 (SeparableConv2D)        (None, 120, 120, 128 35200       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 120, 120, 128 0           conv05[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add02 (Add)                     (None, 120, 120, 128 0           activation_4[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "down_02 (SeparableConv2D)       (None, 60, 60, 256)  34176       add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 60, 60, 256)  0           down_02[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv06 (SeparableConv2D)        (None, 60, 60, 512)  133888      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 60, 60, 512)  0           conv06[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv07 (SeparableConv2D)        (None, 60, 60, 256)  135936      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 60, 60, 256)  0           conv07[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add03 (Add)                     (None, 60, 60, 256)  0           activation_7[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "down_03 (SeparableConv2D)       (None, 30, 30, 512)  133888      add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 30, 30, 512)  0           down_03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv08 (SeparableConv2D)        (None, 30, 30, 1024) 529920      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 30, 30, 1024) 0           conv08[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv09 (SeparableConv2D)        (None, 30, 30, 512)  534016      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 30, 30, 512)  0           conv09[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add04 (Add)                     (None, 30, 30, 512)  0           activation_10[0][0]              \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 60, 60, 128)  16512       add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 60, 60, 128)  32896       add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 30, 30, 256)  65792       add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 30, 30, 256)  131328      add04[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 120, 64) 4160        add01[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 120, 120, 64) 8256        add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 60, 60, 128)  0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 30, 30, 256)  0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 120, 120, 64) 0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 60, 60, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 30, 30, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 120, 120, 64) 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "grid_att02 (Conv2D)             (None, 60, 60, 1)    129         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gird_att03 (Conv2D)             (None, 30, 30, 1)    257         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "grid_att01 (Conv2D)             (None, 120, 120, 1)  65          activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 120, 120, 1)  0           grid_att02[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 60, 60, 1)    0           gird_att03[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 240, 240, 1)  0           grid_att01[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 120, 120, 128 0           add02[0][0]                      \n",
      "                                                                 up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 60, 60, 256)  0           add03[0][0]                      \n",
      "                                                                 up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 240, 240, 64) 0           add01[0][0]                      \n",
      "                                                                 up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 120, 120, 64) 8256        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 60, 60, 64)   16448       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 240, 240, 64) 4160        multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 240, 240, 64) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 240, 240, 64) 0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 240, 240, 192 0           conv2d_6[0][0]                   \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 240, 240, 64) 12352       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 240, 240, 128 0           conv2d_6[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 240, 240, 128 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 240, 240, 128 0           up_sampling2d_4[0][0]            \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 57600, 128)] 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_21 (TensorF [(None, 57600, 128)] 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_37 (TensorF [(None, 57600, 128)] 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_3 (TensorFlo [(None, 128, 57600)] 0           tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 57600, 128)] 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_15 (TensorFl [(None, 128, 57600)] 0           tf_op_layer_Reshape_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_20 (TensorF [(None, 57600, 128)] 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_27 (TensorFl [(None, 128, 57600)] 0           tf_op_layer_Reshape_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_36 (TensorF [(None, 57600, 128)] 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 240, 240, 16) 0           pam01_conv02block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_4 (TensorFlo [(None, 128, 128)]   0           tf_op_layer_Einsum_3[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 240, 240, 16) 0           pam01_conv02block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_16 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_15[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 240, 240, 16) 0           pam01_conv02block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_28 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_27[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 240, 240, 16) 0           pam01_conv01block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 57600, 16)]  0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 57600, 128)] 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock01 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 240, 240, 16) 0           pam01_conv01block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_17 (TensorF [(None, 57600, 16)]  0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_22 (TensorF [(None, 57600, 128)] 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock02 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_16[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 240, 240, 16) 0           pam01_conv01block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_33 (TensorF [(None, 57600, 16)]  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_38 (TensorF [(None, 57600, 128)] 0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock03 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_28[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 57600, 16)]  0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum (TensorFlowO [(None, 16, 57600)]  0           tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block01 (Conv2D)    (None, 240, 240, 128 16512       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_5 (TensorFlo [(None, 57600, 128)] 0           tf_op_layer_Reshape_6[0][0]      \n",
      "                                                                 cam01_softmaxblock01[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_16 (TensorF [(None, 57600, 16)]  0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_12 (TensorFl [(None, 16, 57600)]  0           tf_op_layer_Reshape_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block02 (Conv2D)    (None, 240, 240, 128 16512       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_17 (TensorFl [(None, 57600, 128)] 0           tf_op_layer_Reshape_22[0][0]     \n",
      "                                                                 cam01_softmaxblock02[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_32 (TensorF [(None, 57600, 16)]  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_24 (TensorFl [(None, 16, 57600)]  0           tf_op_layer_Reshape_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block03 (Conv2D)    (None, 240, 240, 128 16512       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_29 (TensorFl [(None, 57600, 128)] 0           tf_op_layer_Reshape_38[0][0]     \n",
      "                                                                 cam01_softmaxblock03[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_1 (TensorFlo [(None, 57600, 57600 0           tf_op_layer_Reshape[0][0]        \n",
      "                                                                 tf_op_layer_Einsum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 240, 240, 128 0           pam01_conv03block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 240, 240, 12 0           tf_op_layer_Einsum_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_13 (TensorFl [(None, 57600, 57600 0           tf_op_layer_Reshape_16[0][0]     \n",
      "                                                                 tf_op_layer_Einsum_12[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 240, 240, 128 0           pam01_conv03block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_23 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_17[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_25 (TensorFl [(None, 57600, 57600 0           tf_op_layer_Reshape_32[0][0]     \n",
      "                                                                 tf_op_layer_Einsum_24[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 240, 240, 128 0           pam01_conv03block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_39 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_29[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock01 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 57600, 128)] 0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock02 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_13[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_18 (TensorF [(None, 57600, 128)] 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 128)          0           tf_op_layer_Reshape_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock03 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_25[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_34 (TensorF [(None, 57600, 128)] 0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 128)          0           tf_op_layer_Reshape_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_2 (TensorFlo [(None, 57600, 128)] 0           pam01_softmaxblock01[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           2064        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_14 (TensorFl [(None, 57600, 128)] 0           pam01_softmaxblock02[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           2064        global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_26 (TensorFl [(None, 57600, 128)] 0           pam01_softmaxblock03[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           2064        global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 240, 240, 12 0           tf_op_layer_Einsum_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock01 (Conv2D)     (None, 240, 240, 1)  129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock01 (Dense)      (None, 128)          2176        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_19 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_14[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock02 (Conv2D)     (None, 240, 240, 1)  129         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock02 (Dense)      (None, 128)          2176        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_35 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_26[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock03 (Conv2D)     (None, 240, 240, 1)  129         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock03 (Dense)      (None, 128)          2176        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_3[0][0]      \n",
      "                                                                 pam01_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_7[0][0]      \n",
      "                                                                 cam01_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_19[0][0]     \n",
      "                                                                 pam01_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_23[0][0]     \n",
      "                                                                 cam01_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_35[0][0]     \n",
      "                                                                 pam01_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_39[0][0]     \n",
      "                                                                 cam01_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock01 (Add)          (None, 240, 240, 128 0           multiply_3[0][0]                 \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock01 (Add)          (None, 240, 240, 128 0           multiply_4[0][0]                 \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock02 (Add)          (None, 240, 240, 128 0           multiply_8[0][0]                 \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock02 (Add)          (None, 240, 240, 128 0           multiply_9[0][0]                 \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock03 (Add)          (None, 240, 240, 128 0           multiply_13[0][0]                \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock03 (Add)          (None, 240, 240, 128 0           multiply_14[0][0]                \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 240, 240, 128 0           pam01_addblock01[0][0]           \n",
      "                                                                 cam01_addblock01[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 240, 240, 128 0           pam01_addblock02[0][0]           \n",
      "                                                                 cam01_addblock02[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 240, 240, 128 0           pam01_addblock03[0][0]           \n",
      "                                                                 cam01_addblock03[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 240, 240, 64) 8256        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 240, 240, 64) 8256        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 240, 240, 64) 8256        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 240, 240, 64) 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 240, 240, 64) 0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 240, 240, 64) 0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 240, 240, 128 0           multiply_5[0][0]                 \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 240, 240, 128 0           multiply_10[0][0]                \n",
      "                                                                 up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 240, 240, 128 0           multiply_15[0][0]                \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_13 (TensorF [(None, 57600, 128)] 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_29 (TensorF [(None, 57600, 128)] 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_45 (TensorF [(None, 57600, 128)] 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_9 (TensorFlo [(None, 128, 57600)] 0           tf_op_layer_Reshape_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_12 (TensorF [(None, 57600, 128)] 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_21 (TensorFl [(None, 128, 57600)] 0           tf_op_layer_Reshape_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_28 (TensorF [(None, 57600, 128)] 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_33 (TensorFl [(None, 128, 57600)] 0           tf_op_layer_Reshape_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_44 (TensorF [(None, 57600, 128)] 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 240, 240, 16) 0           pam02_conv02block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_10 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_9[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 240, 240, 16) 0           pam02_conv02block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_22 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_21[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 240, 240, 16) 0           pam02_conv02block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_34 (TensorFl [(None, 128, 128)]   0           tf_op_layer_Einsum_33[0][0]      \n",
      "                                                                 tf_op_layer_Reshape_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 240, 240, 16) 0           pam02_conv01block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 57600, 16)]  0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_14 (TensorF [(None, 57600, 128)] 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock01 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 240, 240, 16) 0           pam02_conv01block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_25 (TensorF [(None, 57600, 16)]  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_30 (TensorF [(None, 57600, 128)] 0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock02 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_22[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 240, 240, 16) 0           pam02_conv01block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_41 (TensorF [(None, 57600, 16)]  0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_46 (TensorF [(None, 57600, 128)] 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock03 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_34[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 57600, 16)]  0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_6 (TensorFlo [(None, 16, 57600)]  0           tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block01 (Conv2D)    (None, 240, 240, 128 16512       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_11 (TensorFl [(None, 57600, 128)] 0           tf_op_layer_Reshape_14[0][0]     \n",
      "                                                                 cam02_softmaxblock01[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_24 (TensorF [(None, 57600, 16)]  0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_18 (TensorFl [(None, 16, 57600)]  0           tf_op_layer_Reshape_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block02 (Conv2D)    (None, 240, 240, 128 16512       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_23 (TensorFl [(None, 57600, 128)] 0           tf_op_layer_Reshape_30[0][0]     \n",
      "                                                                 cam02_softmaxblock02[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_40 (TensorF [(None, 57600, 16)]  0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_30 (TensorFl [(None, 16, 57600)]  0           tf_op_layer_Reshape_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block03 (Conv2D)    (None, 240, 240, 128 16512       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_35 (TensorFl [(None, 57600, 128)] 0           tf_op_layer_Reshape_46[0][0]     \n",
      "                                                                 cam02_softmaxblock03[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_7 (TensorFlo [(None, 57600, 57600 0           tf_op_layer_Reshape_8[0][0]      \n",
      "                                                                 tf_op_layer_Einsum_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 240, 240, 128 0           pam02_conv03block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_15 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_19 (TensorFl [(None, 57600, 57600 0           tf_op_layer_Reshape_24[0][0]     \n",
      "                                                                 tf_op_layer_Einsum_18[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 240, 240, 128 0           pam02_conv03block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_31 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_23[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_31 (TensorFl [(None, 57600, 57600 0           tf_op_layer_Reshape_40[0][0]     \n",
      "                                                                 tf_op_layer_Einsum_30[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 240, 240, 128 0           pam02_conv03block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_47 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_35[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock01 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_10 (TensorF [(None, 57600, 128)] 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 128)          0           tf_op_layer_Reshape_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock02 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_19[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_26 (TensorF [(None, 57600, 128)] 0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 128)          0           tf_op_layer_Reshape_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock03 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_31[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_42 (TensorF [(None, 57600, 128)] 0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 128)          0           tf_op_layer_Reshape_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_8 (TensorFlo [(None, 57600, 128)] 0           pam02_softmaxblock01[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           2064        global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_20 (TensorFl [(None, 57600, 128)] 0           pam02_softmaxblock02[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_32 (TensorFl [(None, 57600, 128)] 0           pam02_softmaxblock03[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_11 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock01 (Conv2D)     (None, 240, 240, 1)  129         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock01 (Dense)      (None, 128)          2176        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_27 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_20[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock02 (Conv2D)     (None, 240, 240, 1)  129         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock02 (Dense)      (None, 128)          2176        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_43 (TensorF [(None, 240, 240, 12 0           tf_op_layer_Einsum_32[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock03 (Conv2D)     (None, 240, 240, 1)  129         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock03 (Dense)      (None, 128)          2176        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_11[0][0]     \n",
      "                                                                 pam02_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 240, 240, 128 0           tf_op_layer_Reshape_15[0][0]     \n",
      "                                                                 cam02_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_27[0][0]     \n",
      "                                                                 pam02_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_31[0][0]     \n",
      "                                                                 cam02_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_43[0][0]     \n",
      "                                                                 pam02_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 240, 240, 128 0           tf_op_layer_Reshape_47[0][0]     \n",
      "                                                                 cam02_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock01 (Add)          (None, 240, 240, 128 0           multiply_6[0][0]                 \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock01 (Add)          (None, 240, 240, 128 0           multiply_7[0][0]                 \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock02 (Add)          (None, 240, 240, 128 0           multiply_11[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock02 (Add)          (None, 240, 240, 128 0           multiply_12[0][0]                \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock03 (Add)          (None, 240, 240, 128 0           multiply_16[0][0]                \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock03 (Add)          (None, 240, 240, 128 0           multiply_17[0][0]                \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 240, 240, 128 0           pam02_addblock01[0][0]           \n",
      "                                                                 cam02_addblock01[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 240, 240, 128 0           pam02_addblock02[0][0]           \n",
      "                                                                 cam02_addblock02[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 240, 240, 128 0           pam02_addblock03[0][0]           \n",
      "                                                                 cam02_addblock03[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 240, 240, 64) 8256        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 240, 240, 64) 8256        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 240, 240, 64) 8256        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "seg_01 (Conv2D)                 (None, 240, 240, 4)  260         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "seg_02 (Conv2D)                 (None, 240, 240, 4)  260         up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seg_03 (Conv2D)                 (None, 240, 240, 4)  260         up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga01 (Conv2D)               (None, 240, 240, 4)  260         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga02 (Conv2D)               (None, 240, 240, 4)  260         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga03 (Conv2D)               (None, 240, 240, 4)  260         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 240, 240, 4)  0           seg_01[0][0]                     \n",
      "                                                                 seg_02[0][0]                     \n",
      "                                                                 seg_03[0][0]                     \n",
      "                                                                 seg_ga01[0][0]                   \n",
      "                                                                 seg_ga02[0][0]                   \n",
      "                                                                 seg_ga03[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 240, 240, 4) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_1 (TensorFl [(None, 240, 240, 4) 0           tf_op_layer_RealDiv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 240, 240, 4)  0           tf_op_layer_RealDiv_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,103,681\n",
      "Trainable params: 2,103,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
