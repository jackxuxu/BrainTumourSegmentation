{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from utils import dice_coef, ss_metric, compute_metric\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue => Label 1 (Necrotic and Non-enhancing Tumor Core)\n",
    "- Yellow => Label 2 (Peritumoral Edema)\n",
    "- Green => Label 3/4 (GD-Enhancing Tumor)\n",
    "---\n",
    "* Core => Label 1 & 3\n",
    "* Enhancing => Label 3\n",
    "* Complete => Label 1,2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "max_epochs = 2\n",
    "BATCH_SIZE = 24\n",
    "lr = 0.000001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = 'self_attention_01' #save version \n",
    "dropout=0.3 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/'\n",
    "stack_npy = \"/home/kevinteng/Desktop/ssd02/BraTS2020_stack03/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def generalized_dice(y_true, y_pred, smooth = 1e-5):\n",
    "    \"\"\"\n",
    "    Generalized Dice Score\n",
    "    https://arxiv.org/pdf/1707.03237\n",
    "    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true    = tf.reshape(y_true,shape=(-1,4))\n",
    "    y_pred    = tf.reshape(y_pred,shape=(-1,4))\n",
    "    sum_p     = tf.reduce_sum(y_pred, -2)\n",
    "    sum_r     = tf.reduce_sum(y_true, -2)\n",
    "    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)\n",
    "    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)\n",
    "    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))\n",
    "    return generalized_dice\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred):   \n",
    "    return 1-generalized_dice(y_true, y_pred)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    The final loss function consists of the summation of two losses \"GDL\" and \"CE\"\n",
    "    with a regularization term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)\n",
    "\n",
    "def data_aug(imgs, seed=8888):\n",
    "    x = tf.image.random_flip_up_down(imgs,seed)\n",
    "    x = tf.image.random_flip_left_right(x,seed)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layer Names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# template for guided attention block\n",
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import conv_block, coordconv_block, up, pool, attention_block\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Activation, Add, Multiply, GaussianNoise\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, Dropout, Softmax, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Layer, Softmax\n",
    "from coord_conv import CoordConv\n",
    "\n",
    "def conv_block_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert type(filters)==list, \"Please input filters of type list.\"\n",
    "    assert type(layer_name)==list, \"Please input filters of type list.\"\n",
    "    x = SeparableConv2D(filters[0], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[0])(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    x = SeparableConv2D(filters[1], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[1])(x)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = SeparableConv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def down_sampling_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None, mode ='coord', x_dim=None, y_dim=None):\n",
    "    assert mode=='coord' or mode=='normal', \"Use 'coord' or 'normal' for mode!\"\n",
    "    if mode=='coord':\n",
    "        #seperable coordconv\n",
    "        assert (x_dim!=None and y_dim!=None), \"Please input dimension for CoordConv!\"\n",
    "        x = Conv2D(1, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer)(x_in)\n",
    "        x = CoordConv(x_dim=x_dim, y_dim=y_dim, with_r=False, filters=filters, strides=(1,1), \n",
    "                      kernel_size = 3, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x)\n",
    "    else:\n",
    "        #normal mode\n",
    "        x = SeparableConv2D(filters, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def res_block_sep(x_in, filters,  layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert len(filters)==2, \"Please assure that there is 3 values for filters.\"\n",
    "    assert len(layer_name)==3, \"Please assure that there is 3 values for layer name\"\n",
    "    layer_name_conv = [layer_name[i] for i in range(len(layer_name)-1)]\n",
    "    output_conv_block = conv_block_sep(x_in, filters, layer_name_conv, batch_norm=batch_norm, kernel_size=kernel_size,\n",
    "                                   kernel_initializer = kernel_initializer, acti = acti, dropout_rate=dropout_rate)\n",
    "    output_add = Add(name = layer_name[-1])([output_conv_block, x_in])\n",
    "    return output_add\n",
    "\n",
    "def attention_block(input_signal, gated_signal, filters, att_layer_name):\n",
    "    #input signal feature maps\n",
    "    is_fm = Conv2D(filters, kernel_size=(1,1), strides=(2, 2), padding = 'same')(input_signal)\n",
    "    #gated signal feature maps\n",
    "    gs_fm = Conv2D(filters, kernel_size=(1,1), strides=(1, 1), padding = 'same')(gated_signal)\n",
    "    #debugger\n",
    "    assert is_fm.shape!=gs_fm.shape, \"Feature maps shape doesn't match!\"\n",
    "    #element wise sum\n",
    "    add = Add()([is_fm, gs_fm])\n",
    "    acti = Activation('relu')(add)\n",
    "    #downsampled attention coefficient\n",
    "    bottle_neck = Conv2D(1, kernel_size=(1,1), activation='sigmoid', name=att_layer_name)(acti)\n",
    "    #bilinear interpolation to get attention coeffcient\n",
    "    alpha = UpSampling2D(interpolation='bilinear')(bottle_neck)\n",
    "    #filter off input signal's features with attention coefficient\n",
    "    multi = Multiply()([input_signal, alpha])\n",
    "    return multi\n",
    "\n",
    "class att_var(Layer):\n",
    "    '''\n",
    "    Attention variable\n",
    "    '''\n",
    "    def __init__(self, initial_val):\n",
    "        super(att_var, self).__init__()\n",
    "        self.initial_val = initial_val\n",
    "    def __call__(self):\n",
    "        alpha = tf.Variable(initial_value=self.initial_val, trainable=True)\n",
    "        return alpha\n",
    "\n",
    "def PAM(inp_feature, layer_name, kernel_initializer='glorot_uniform', acti='relu'):\n",
    "    '''\n",
    "    Position attention module\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param layer_name: List of layer names\n",
    "    [1st conv block, 2nd conv block, softmax output, 3rd conv block, position coefficient, Add output]\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]\n",
    "    :return: PAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #scale down ratio\n",
    "    c_8 = c//8\n",
    "    #\n",
    "    assert len(layer_name)>=5, 'Layer list length should be 5!'\n",
    "    # Branch01 Dimension: [w,h,c/8] => [(wxh),c/8]\n",
    "    query = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[0], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "            kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    query = tf.reshape(query,[-1,(w*h),c_8 ])\n",
    "    # Branch02 Dimension: [w,h,c/8] => [c/8,(wxh)]\n",
    "    key = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[1], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    key = tf.reshape(key, [-1,(w*h),c_8 ])\n",
    "    key = tf.einsum('bij->bji', key) #transpose/permutation\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0102 = tf.einsum('bij,bjk->bik', query, key) #[(wxh),(wxh)]\n",
    "    softmax0102 = Softmax(name=layer_name[2])(matmul_0102)\n",
    "    # Branch03\n",
    "    value = conv_2d(inp_feature, filters=c, layer_name=layer_name[3], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    value = tf.reshape(value,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('bij,bjk->bik',softmax0102,value) #[(wxh),c]\n",
    "    # Output\n",
    "    output = tf.reshape(matmul_all, [-1,w,h,c]) #[w,h,c]\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    lambda_p = Conv2D(filters=1,kernel_size=1,padding='same',activation='sigmoid', name=layer_name[4])(inp_feature)\n",
    "    output = Multiply()([output, lambda_p])\n",
    "    output_add = Add(name = layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def CAM(inp_feature, layer_name):\n",
    "    '''\n",
    "    Channel attention\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]k\n",
    "    :param layer_name: List of layer names\n",
    "        [softmax output, channel attention coefficients, Add output]\n",
    "    :return: CAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    assert len(layer_name)>=2, 'Layer list length should be 2!'\n",
    "    # Branch01 Dimension: [w,h,c] => [(wxh),c]\n",
    "    query = tf.reshape(inp_feature, [-1,(w*h),c])\n",
    "    # Branch02 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    key = tf.reshape(inp_feature, [-1,(w*h),c]) #[(wxh),c]\n",
    "    key = tf.einsum('ijk->ikj', key) #Permute:[c,(wxh)]\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0201 = tf.einsum('ijk,ikl->ijl', key, query) #[c,c]\n",
    "    softmax0102 = Softmax(name=layer_name[0])(matmul_0201)\n",
    "    # Branch03 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    value = tf.reshape(inp_feature,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('ijk,ikl->ijl', value,softmax0102) #[(wxh),c]\n",
    "    #output\n",
    "    output = tf.reshape(matmul_all,[-1,w,h,c])#[w,h,c]\n",
    "    #provides learnable parameter\n",
    "    #*channel wise attention, inspired by Squeeze Excitation(SE) block\n",
    "    GAP = GlobalAveragePooling2D()(output)\n",
    "    dense01 = Dense(c//8, activation='relu')(GAP)\n",
    "    alpha_c = Dense(c,activation='sigmoid', name=layer_name[1])(dense01)\n",
    "    #outputs\n",
    "    output = Multiply()([output, alpha_c])\n",
    "    output_add = Add(name=layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def guided_attention_block(inp_feature, layer_name_p, layer_name_c):\n",
    "    '''\n",
    "    Guided attention block that takes feature as input and concatenates features\n",
    "    from PAM and CAM as output\n",
    "    :param inp_feature: Input features\n",
    "    :param layer_name_p: layer name list for PAM\n",
    "    :param layer_name_c: layer name list for CAM\n",
    "    :return: squeezed concatenated features of PAM and CAM\n",
    "    '''\n",
    "    pam_feature = PAM(inp_feature, layer_name_p, kernel_initializer=hn)\n",
    "    cam_feature = CAM(inp_feature, layer_name_c)\n",
    "    add = Add()([pam_feature,cam_feature])\n",
    "    squeeze = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn,\n",
    "                       activation='relu')(add)\n",
    "    return squeeze\n",
    "\n",
    "def guided_attention(res_feature, ms_feature, layer_name):\n",
    "    '''\n",
    "    Guided attention module\n",
    "    :param res_feature: Upsampled Feature maps from Res Block\n",
    "    :param ms_feature: Multi scale feature maps result from Res Block\n",
    "    :param layer_name: Layer Name should consist be a list contating 4 list\n",
    "    Example:\n",
    "    layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                      'pam01_alpha','pam01_add']\n",
    "    layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "    layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                      'pam02_alpha', 'pam02_add']\n",
    "    layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "    layer_name = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "    :return: guided attention module with shape same as input\n",
    "    '''\n",
    "    assert len(layer_name)==4, \"Layer name should be a list consisting 4 lists!\"\n",
    "    #self attention block01\n",
    "    concat01 = concatenate([res_feature, ms_feature], axis=-1)\n",
    "    squeeze01 = guided_attention_block(concat01, layer_name[0], layer_name[1])\n",
    "    multi01 = Multiply()([squeeze01, ms_feature])\n",
    "    #self attention block02\n",
    "    concat02 = concatenate([multi01, res_feature])\n",
    "    squeeze02 = guided_attention_block(concat02, layer_name[2], layer_name[3])\n",
    "    return squeeze02\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Resnet as backbone for multiscale feature retrieval.\n",
    "    Each resblock output(input signal), next resblock output(gated signal) is\n",
    "    feed into the gated attention for multi scale feature refinement.\n",
    "    Each gated attention output is pass through a bottle neck layer to standardize\n",
    "    the channel size by squashing them to desired filter size of 64.\n",
    "    The features are upsampled at each block to the corresponding [wxh] dimension\n",
    "    of w:240, h:240.\n",
    "    The upsampled features are concat and squash to corresponding channel size of 64\n",
    "    which yield multiscale feature.\n",
    "    :param x: batched images\n",
    "    :return: feature maps of each res block\n",
    "    '''\n",
    "    #inject noise\n",
    "    gauss1 = GaussianNoise(0.01)(x)\n",
    "    #---- ResNet and Multiscale Features----\n",
    "    #1st block\n",
    "    conv01 = CoordConv(x_dim=240, y_dim=240, with_r=False, filters=64, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=hn, name='conv01')(gauss1)\n",
    "    res_block01 = res_block_sep(conv01, filters=[128, 64], layer_name=[\"conv02\", \"conv03\", \"add01\"])\n",
    "    #2nd block\n",
    "    down_01 = down_sampling_sep(res_block01, filters=128, layer_name = 'down_01',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=120, y_dim=120)\n",
    "    res_block02 = res_block_sep(down_01, filters=[256, 128], layer_name=[\"conv04\", \"conv05\", \"add02\"])\n",
    "    #3rd block\n",
    "    down_02 = down_sampling_sep(res_block02, filters=256, layer_name = 'down_02',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=60, y_dim=60)\n",
    "    res_block03 = res_block_sep(down_02, filters=[512, 256], layer_name=[\"conv06\", \"conv07\", \"add03\"])\n",
    "    #4th block\n",
    "    down_03 = down_sampling_sep(res_block03, filters=512, layer_name = 'down_03',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=30, y_dim=30)\n",
    "    res_block04 = res_block_sep(down_03, filters=[1024, 512], layer_name=[\"conv08\", \"conv09\", \"add04\"])\n",
    "    #grid attention blocks\n",
    "    att_block01 = attention_block(res_block01,res_block02,64,'grid_att01')\n",
    "    att_block02 = attention_block(res_block02,res_block03,128,'grid_att02')\n",
    "    att_block03 = attention_block(res_block03, res_block04,256,'gird_att03')\n",
    "    #bottle neck => layer squash all attention block to same filter size 64\n",
    "    bottle01 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block01)\n",
    "    bottle02 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block02)\n",
    "    bottle03 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block03)\n",
    "    #upsampling for all layers to same (wxh) dimension=>240x240\n",
    "    up01 = bottle01 #[240,240,64]\n",
    "    up02 = UpSampling2D(size=(2, 2), interpolation='bilinear')(bottle02) #[120,120,64]=>[240,240,64]\n",
    "    up03 = UpSampling2D(size=(4,4), interpolation='bilinear')(bottle03) #[60,60,64]=>[240,240,64]\n",
    "    #multiscale features\n",
    "    concat_all = concatenate([up01,up02,up03],axis=-1) #[240,240,3*64]\n",
    "    #squeeze to have the same channel as upsampled features [240,240,3*64] => [240,240,64]\n",
    "    ms_feature = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(concat_all)\n",
    "    #Segmentations from multiscale features *without softmax activation\n",
    "    seg_01 = Conv2D(4, (1,1), name='seg_01')(up01)\n",
    "    seg_02 = Conv2D(4, (1,1), name='seg_02')(up02)\n",
    "    seg_03 = Conv2D(4, (1,1), name='seg_03')(up02)\n",
    "\n",
    "    #----self guided attention blocks-----\n",
    "    ga_01 = guided_attention(up01, ms_feature, layer_name_ga[0])\n",
    "    ga_02 = guided_attention(up02, ms_feature, layer_name_ga[1])\n",
    "    ga_03 = guided_attention(up03, ms_feature, layer_name_ga[2])\n",
    "    #Segmentations from guided attention features *without softmax activation\n",
    "    seg_ga01 = Conv2D(4, (1,1), name='seg_ga01')(ga_01)\n",
    "    seg_ga02 = Conv2D(4, (1,1), name='seg_ga02')(ga_02)\n",
    "    seg_ga03 = Conv2D(4, (1,1), name='seg_ga03')(ga_03)\n",
    "    #outputs for xent losses\n",
    "    output_xent = [seg_01, seg_02, seg_03, seg_ga01, seg_ga02, seg_ga03]\n",
    "    #output for dice coefficient loss\n",
    "    pred_seg = Add()(output_xent)\n",
    "    pred_seg = pred_seg/len(output_xent)\n",
    "    output_dice = Softmax()(pred_seg/len(output_xent))\n",
    "    return output_xent, output_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "#Build Model\n",
    "input_layer = Input(shape=(240,240,4))\n",
    "model = Model(input_layer, forward(input_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_45\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_34 (InputLayer)           [(None, 240, 240, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_11 (GaussianNois (None, 240, 240, 4)  0           input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "coord_conv_11 (CoordConv)       (None, 240, 240, 64) 3520        gaussian_noise_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv02 (SeparableConv2D)        (None, 240, 240, 128 8896        coord_conv_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 240, 240, 128 0           conv02[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv03 (SeparableConv2D)        (None, 240, 240, 64) 9408        activation_382[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 240, 240, 64) 0           conv03[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add01 (Add)                     (None, 240, 240, 64) 0           activation_383[0][0]             \n",
      "                                                                 coord_conv_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "down_01 (SeparableConv2D)       (None, 120, 120, 128 8896        add01[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 120, 120, 128 0           down_01[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv04 (SeparableConv2D)        (None, 120, 120, 256 34176       activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 120, 120, 256 0           conv04[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv05 (SeparableConv2D)        (None, 120, 120, 128 35200       activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 120, 120, 128 0           conv05[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add02 (Add)                     (None, 120, 120, 128 0           activation_386[0][0]             \n",
      "                                                                 activation_384[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "down_02 (SeparableConv2D)       (None, 60, 60, 256)  34176       add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 60, 60, 256)  0           down_02[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv06 (SeparableConv2D)        (None, 60, 60, 512)  133888      activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 60, 60, 512)  0           conv06[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv07 (SeparableConv2D)        (None, 60, 60, 256)  135936      activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 60, 60, 256)  0           conv07[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add03 (Add)                     (None, 60, 60, 256)  0           activation_389[0][0]             \n",
      "                                                                 activation_387[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "down_03 (SeparableConv2D)       (None, 30, 30, 512)  133888      add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 30, 30, 512)  0           down_03[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv08 (SeparableConv2D)        (None, 30, 30, 1024) 529920      activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 30, 30, 1024) 0           conv08[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv09 (SeparableConv2D)        (None, 30, 30, 512)  534016      activation_391[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 30, 30, 512)  0           conv09[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add04 (Add)                     (None, 30, 30, 512)  0           activation_392[0][0]             \n",
      "                                                                 activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 60, 60, 128)  16512       add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 60, 60, 128)  32896       add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 30, 30, 256)  65792       add03[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 30, 30, 256)  131328      add04[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 120, 120, 64) 4160        add01[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 120, 120, 64) 8256        add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_110 (Add)                   (None, 60, 60, 128)  0           conv2d_187[0][0]                 \n",
      "                                                                 conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_111 (Add)                   (None, 30, 30, 256)  0           conv2d_189[0][0]                 \n",
      "                                                                 conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_109 (Add)                   (None, 120, 120, 64) 0           conv2d_185[0][0]                 \n",
      "                                                                 conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 60, 60, 128)  0           add_110[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 30, 30, 256)  0           add_111[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 120, 120, 64) 0           add_109[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "grid_att02 (Conv2D)             (None, 60, 60, 1)    129         activation_394[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gird_att03 (Conv2D)             (None, 30, 30, 1)    257         activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "grid_att01 (Conv2D)             (None, 120, 120, 1)  65          activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_56 (UpSampling2D) (None, 120, 120, 1)  0           grid_att02[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_57 (UpSampling2D) (None, 60, 60, 1)    0           gird_att03[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_55 (UpSampling2D) (None, 240, 240, 1)  0           grid_att01[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_212 (Multiply)         (None, 120, 120, 128 0           add02[0][0]                      \n",
      "                                                                 up_sampling2d_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_213 (Multiply)         (None, 60, 60, 256)  0           add03[0][0]                      \n",
      "                                                                 up_sampling2d_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_211 (Multiply)         (None, 240, 240, 64) 0           add01[0][0]                      \n",
      "                                                                 up_sampling2d_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 120, 120, 64) 8256        multiply_212[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 60, 60, 64)   16448       multiply_213[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 240, 240, 64) 4160        multiply_211[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_58 (UpSampling2D) (None, 240, 240, 64) 0           conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_59 (UpSampling2D) (None, 240, 240, 64) 0           conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_79 (Concatenate)    (None, 240, 240, 192 0           conv2d_191[0][0]                 \n",
      "                                                                 up_sampling2d_58[0][0]           \n",
      "                                                                 up_sampling2d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 240, 240, 64) 12352       concatenate_79[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_80 (Concatenate)    (None, 240, 240, 128 0           conv2d_191[0][0]                 \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_82 (Concatenate)    (None, 240, 240, 128 0           up_sampling2d_58[0][0]           \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_84 (Concatenate)    (None, 240, 240, 128 0           up_sampling2d_59[0][0]           \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_613 (Tensor [(None, 57600, 128)] 0           concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_629 (Tensor [(None, 57600, 128)] 0           concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_645 (Tensor [(None, 57600, 128)] 0           concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_459 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_612 (Tensor [(None, 57600, 128)] 0           concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_471 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_628 (Tensor [(None, 57600, 128)] 0           concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv02block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_483 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_644 (Tensor [(None, 57600, 128)] 0           concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 240, 240, 16) 0           pam01_conv02block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_460 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_459[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 240, 240, 16) 0           pam01_conv02block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_472 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_471[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv01block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 240, 240, 16) 0           pam01_conv02block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_484 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_483[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 240, 240, 16) 0           pam01_conv01block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_609 (Tensor [(None, 57600, 16)]  0           activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_614 (Tensor [(None, 57600, 128)] 0           concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock01 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_460[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 240, 240, 16) 0           pam01_conv01block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_625 (Tensor [(None, 57600, 16)]  0           activation_403[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_630 (Tensor [(None, 57600, 128)] 0           concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock02 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_472[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 240, 240, 16) 0           pam01_conv01block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_641 (Tensor [(None, 57600, 16)]  0           activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_646 (Tensor [(None, 57600, 128)] 0           concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_softmaxblock03 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_484[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_608 (Tensor [(None, 57600, 16)]  0           activation_396[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_456 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block01 (Conv2D)    (None, 240, 240, 128 16512       concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_461 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_614[0][0]    \n",
      "                                                                 cam01_softmaxblock01[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_624 (Tensor [(None, 57600, 16)]  0           activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_468 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block02 (Conv2D)    (None, 240, 240, 128 16512       concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_473 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_630[0][0]    \n",
      "                                                                 cam01_softmaxblock02[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_640 (Tensor [(None, 57600, 16)]  0           activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_480 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_conv03block03 (Conv2D)    (None, 240, 240, 128 16512       concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_485 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_646[0][0]    \n",
      "                                                                 cam01_softmaxblock03[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_457 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_608[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_456[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 240, 240, 128 0           pam01_conv03block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_615 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_461[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_469 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_624[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_468[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 240, 240, 128 0           pam01_conv03block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_631 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_473[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_481 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_640[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_480[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 240, 240, 128 0           pam01_conv03block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_647 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_485[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock01 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_457[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_610 (Tensor [(None, 57600, 128)] 0           activation_398[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_74 (Gl (None, 128)          0           tf_op_layer_Reshape_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock02 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_469[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_626 (Tensor [(None, 57600, 128)] 0           activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_76 (Gl (None, 128)          0           tf_op_layer_Reshape_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam01_softmaxblock03 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_481[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_642 (Tensor [(None, 57600, 128)] 0           activation_410[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_78 (Gl (None, 128)          0           tf_op_layer_Reshape_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_458 (TensorF [(None, 57600, 128)] 0           pam01_softmaxblock01[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 16)           2064        global_average_pooling2d_74[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_470 (TensorF [(None, 57600, 128)] 0           pam01_softmaxblock02[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_81 (Dense)                (None, 16)           2064        global_average_pooling2d_76[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_482 (TensorF [(None, 57600, 128)] 0           pam01_softmaxblock03[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_83 (Dense)                (None, 16)           2064        global_average_pooling2d_78[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_611 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_458[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock01 (Conv2D)     (None, 240, 240, 1)  129         concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock01 (Dense)      (None, 128)          2176        dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_627 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_470[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock02 (Conv2D)     (None, 240, 240, 1)  129         concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock02 (Dense)      (None, 128)          2176        dense_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_643 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_482[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam01_alphablock03 (Conv2D)     (None, 240, 240, 1)  129         concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_alphablock03 (Dense)      (None, 128)          2176        dense_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_214 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_611[0][0]    \n",
      "                                                                 pam01_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_215 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_615[0][0]    \n",
      "                                                                 cam01_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_219 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_627[0][0]    \n",
      "                                                                 pam01_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_220 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_631[0][0]    \n",
      "                                                                 cam01_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_224 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_643[0][0]    \n",
      "                                                                 pam01_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_225 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_647[0][0]    \n",
      "                                                                 cam01_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock01 (Add)          (None, 240, 240, 128 0           multiply_214[0][0]               \n",
      "                                                                 concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock01 (Add)          (None, 240, 240, 128 0           multiply_215[0][0]               \n",
      "                                                                 concatenate_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock02 (Add)          (None, 240, 240, 128 0           multiply_219[0][0]               \n",
      "                                                                 concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock02 (Add)          (None, 240, 240, 128 0           multiply_220[0][0]               \n",
      "                                                                 concatenate_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam01_addblock03 (Add)          (None, 240, 240, 128 0           multiply_224[0][0]               \n",
      "                                                                 concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam01_addblock03 (Add)          (None, 240, 240, 128 0           multiply_225[0][0]               \n",
      "                                                                 concatenate_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_112 (Add)                   (None, 240, 240, 128 0           pam01_addblock01[0][0]           \n",
      "                                                                 cam01_addblock01[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_114 (Add)                   (None, 240, 240, 128 0           pam01_addblock02[0][0]           \n",
      "                                                                 cam01_addblock02[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_116 (Add)                   (None, 240, 240, 128 0           pam01_addblock03[0][0]           \n",
      "                                                                 cam01_addblock03[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 240, 240, 64) 8256        add_112[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 240, 240, 64) 8256        add_114[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 240, 240, 64) 8256        add_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_216 (Multiply)         (None, 240, 240, 64) 0           conv2d_195[0][0]                 \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_221 (Multiply)         (None, 240, 240, 64) 0           conv2d_197[0][0]                 \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_226 (Multiply)         (None, 240, 240, 64) 0           conv2d_199[0][0]                 \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_81 (Concatenate)    (None, 240, 240, 128 0           multiply_216[0][0]               \n",
      "                                                                 conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_83 (Concatenate)    (None, 240, 240, 128 0           multiply_221[0][0]               \n",
      "                                                                 up_sampling2d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 240, 240, 128 0           multiply_226[0][0]               \n",
      "                                                                 up_sampling2d_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_621 (Tensor [(None, 57600, 128)] 0           concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_637 (Tensor [(None, 57600, 128)] 0           concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_653 (Tensor [(None, 57600, 128)] 0           concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_465 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_620 (Tensor [(None, 57600, 128)] 0           concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_477 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_636 (Tensor [(None, 57600, 128)] 0           concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv02block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_489 (TensorF [(None, 128, 57600)] 0           tf_op_layer_Reshape_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_652 (Tensor [(None, 57600, 128)] 0           concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block01 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 240, 240, 16) 0           pam02_conv02block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_466 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_465[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block02 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 240, 240, 16) 0           pam02_conv02block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_478 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_477[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv01block03 (Conv2D)    (None, 240, 240, 16) 2064        concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 240, 240, 16) 0           pam02_conv02block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_490 (TensorF [(None, 128, 128)]   0           tf_op_layer_Einsum_489[0][0]     \n",
      "                                                                 tf_op_layer_Reshape_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 240, 240, 16) 0           pam02_conv01block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_617 (Tensor [(None, 57600, 16)]  0           activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_622 (Tensor [(None, 57600, 128)] 0           concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock01 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_466[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 240, 240, 16) 0           pam02_conv01block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_633 (Tensor [(None, 57600, 16)]  0           activation_406[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_638 (Tensor [(None, 57600, 128)] 0           concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock02 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_478[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 240, 240, 16) 0           pam02_conv01block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_649 (Tensor [(None, 57600, 16)]  0           activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_654 (Tensor [(None, 57600, 128)] 0           concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_softmaxblock03 (Softmax)  (None, 128, 128)     0           tf_op_layer_Einsum_490[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_616 (Tensor [(None, 57600, 16)]  0           activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_462 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block01 (Conv2D)    (None, 240, 240, 128 16512       concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_467 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_622[0][0]    \n",
      "                                                                 cam02_softmaxblock01[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_632 (Tensor [(None, 57600, 16)]  0           activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_474 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block02 (Conv2D)    (None, 240, 240, 128 16512       concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_479 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_638[0][0]    \n",
      "                                                                 cam02_softmaxblock02[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_648 (Tensor [(None, 57600, 16)]  0           activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_486 (TensorF [(None, 16, 57600)]  0           tf_op_layer_Reshape_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_conv03block03 (Conv2D)    (None, 240, 240, 128 16512       concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_491 (TensorF [(None, 57600, 128)] 0           tf_op_layer_Reshape_654[0][0]    \n",
      "                                                                 cam02_softmaxblock03[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_463 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_616[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_462[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 240, 240, 128 0           pam02_conv03block01[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_623 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_467[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_475 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_632[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_474[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 240, 240, 128 0           pam02_conv03block02[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_639 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_479[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_487 (TensorF [(None, 57600, 57600 0           tf_op_layer_Reshape_648[0][0]    \n",
      "                                                                 tf_op_layer_Einsum_486[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 240, 240, 128 0           pam02_conv03block03[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_655 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_491[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock01 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_463[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_618 (Tensor [(None, 57600, 128)] 0           activation_401[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_75 (Gl (None, 128)          0           tf_op_layer_Reshape_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock02 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_475[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_634 (Tensor [(None, 57600, 128)] 0           activation_407[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_77 (Gl (None, 128)          0           tf_op_layer_Reshape_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "pam02_softmaxblock03 (Softmax)  (None, 57600, 57600) 0           tf_op_layer_Einsum_487[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_650 (Tensor [(None, 57600, 128)] 0           activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_79 (Gl (None, 128)          0           tf_op_layer_Reshape_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_464 (TensorF [(None, 57600, 128)] 0           pam02_softmaxblock01[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 16)           2064        global_average_pooling2d_75[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_476 (TensorF [(None, 57600, 128)] 0           pam02_softmaxblock02[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_82 (Dense)                (None, 16)           2064        global_average_pooling2d_77[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Einsum_488 (TensorF [(None, 57600, 128)] 0           pam02_softmaxblock03[0][0]       \n",
      "                                                                 tf_op_layer_Reshape_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_84 (Dense)                (None, 16)           2064        global_average_pooling2d_79[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_619 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_464[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock01 (Conv2D)     (None, 240, 240, 1)  129         concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock01 (Dense)      (None, 128)          2176        dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_635 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_476[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock02 (Conv2D)     (None, 240, 240, 1)  129         concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock02 (Dense)      (None, 128)          2176        dense_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_651 (Tensor [(None, 240, 240, 12 0           tf_op_layer_Einsum_488[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "pam02_alphablock03 (Conv2D)     (None, 240, 240, 1)  129         concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_alphablock03 (Dense)      (None, 128)          2176        dense_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_217 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_619[0][0]    \n",
      "                                                                 pam02_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_218 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_623[0][0]    \n",
      "                                                                 cam02_alphablock01[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_222 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_635[0][0]    \n",
      "                                                                 pam02_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_223 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_639[0][0]    \n",
      "                                                                 cam02_alphablock02[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_227 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_651[0][0]    \n",
      "                                                                 pam02_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_228 (Multiply)         (None, 240, 240, 128 0           tf_op_layer_Reshape_655[0][0]    \n",
      "                                                                 cam02_alphablock03[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock01 (Add)          (None, 240, 240, 128 0           multiply_217[0][0]               \n",
      "                                                                 concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock01 (Add)          (None, 240, 240, 128 0           multiply_218[0][0]               \n",
      "                                                                 concatenate_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock02 (Add)          (None, 240, 240, 128 0           multiply_222[0][0]               \n",
      "                                                                 concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock02 (Add)          (None, 240, 240, 128 0           multiply_223[0][0]               \n",
      "                                                                 concatenate_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "pam02_addblock03 (Add)          (None, 240, 240, 128 0           multiply_227[0][0]               \n",
      "                                                                 concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cam02_addblock03 (Add)          (None, 240, 240, 128 0           multiply_228[0][0]               \n",
      "                                                                 concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_113 (Add)                   (None, 240, 240, 128 0           pam02_addblock01[0][0]           \n",
      "                                                                 cam02_addblock01[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_115 (Add)                   (None, 240, 240, 128 0           pam02_addblock02[0][0]           \n",
      "                                                                 cam02_addblock02[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_117 (Add)                   (None, 240, 240, 128 0           pam02_addblock03[0][0]           \n",
      "                                                                 cam02_addblock03[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 240, 240, 64) 8256        add_113[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 240, 240, 64) 8256        add_115[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 240, 240, 64) 8256        add_117[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "seg_01 (Conv2D)                 (None, 240, 240, 4)  260         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "seg_02 (Conv2D)                 (None, 240, 240, 4)  260         up_sampling2d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seg_03 (Conv2D)                 (None, 240, 240, 4)  260         up_sampling2d_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga01 (Conv2D)               (None, 240, 240, 4)  260         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga02 (Conv2D)               (None, 240, 240, 4)  260         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "seg_ga03 (Conv2D)               (None, 240, 240, 4)  260         conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_118 (Add)                   (None, 240, 240, 4)  0           seg_01[0][0]                     \n",
      "                                                                 seg_02[0][0]                     \n",
      "                                                                 seg_03[0][0]                     \n",
      "                                                                 seg_ga01[0][0]                   \n",
      "                                                                 seg_ga02[0][0]                   \n",
      "                                                                 seg_ga03[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_5 (TensorFl [(None, 240, 240, 4) 0           add_118[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv_6 (TensorFl [(None, 240, 240, 4) 0           tf_op_layer_RealDiv_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "softmax_6 (Softmax)             (None, 240, 240, 4)  0           tf_op_layer_RealDiv_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,103,681\n",
      "Trainable params: 2,103,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_xent, output_dice = model(image, training=True)\n",
    "        loss_dice = generalized_dice_loss(label, output_dice)\n",
    "        loss_xents=[]\n",
    "        for seg in output_xent:\n",
    "            loss_xent = xent_logit(label, seg)\n",
    "            loss_xents.append(loss_xent)\n",
    "        loss_total = sum(loss_xents)+loss_dice\n",
    "    gradients = tape.gradient(loss_total, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return output_dice, loss_total, gradients\n",
    "\n",
    "@tf.function\n",
    "def val_fn(image, label):\n",
    "    model_output = model(image, training=False)\n",
    "    loss = custom_loss(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "#list\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "loss_inner = []\n",
    "while epochs <= max_epochs:\n",
    "    start = time.time()\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    ds = os.listdir(tfrecords_read_dir)\n",
    "    #shuffle directory list of tfrecords\n",
    "    shuffle = random.shuffle(ds)\n",
    "    for tf_re in ds:\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        acc_inner = []\n",
    "        for imgs in dataset:\n",
    "            #data augmentation\n",
    "            imgs = data_aug(imgs)\n",
    "            image = imgs[:,:,:,:4]\n",
    "            #unprocessed label for plotting\n",
    "            label = imgs[:,:,:,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            img_seg, loss, gradients = train_fn(image, label) #training function\n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32)\n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            #append accuracy for every steps\n",
    "            acc_inner.append(acc)\n",
    "            if epochs%5==0:\n",
    "                dc_list, sens_list, spec_list =compute_metric(label,img_seg)\n",
    "                dc_app.append(dc_list)\n",
    "                sens_app.append(sens_list)\n",
    "                spec_app.append(spec_list)\n",
    "            #output\n",
    "            if steps%1==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                loss_list.append(loss)\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp, sens_list_stp, spec_list_stp =compute_metric(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Sensitivity: {}\".format(sens_list_stp))\n",
    "                print(\"Specificity: {}\".format(spec_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "        acc_list.append(np.mean(acc_inner))\n",
    "    if epochs%5==0:\n",
    "        mean_dc = np.mean(np.array(dc_app),0)\n",
    "        mean_sens = np.mean(np.array(sens_app),0)\n",
    "        mean_spec = np.mean(np.array(spec_app),0)\n",
    "        print()\n",
    "        print('-----------<Summary for Epoch:{}>------------'.format(epochs))\n",
    "        print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "        #'core','enhancing','complete'\n",
    "        print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "        print(\"Mean Sensitivity: {}\".format(mean_sens))\n",
    "        print(\"Mean Specificity: {}\".format(mean_spec))\n",
    "        print('------------------------------------------------')\n",
    "        print()\n",
    "    elapsed_time =(time.time()-start)/60 #unit in mins\n",
    "    print(\"Compute time per epochs: {:.2f} mins\".format(elapsed_time))\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "def output_fn(image):\n",
    "    model.trainable = False\n",
    "    model_output = model(image)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds = '/home/kevinteng/Desktop/ssd02/BraTS2020_preprocessed03/'\n",
    "# save_path = '/home/kevinteng/Desktop/ssd02/submission/'\n",
    "# actual_label = '/home/kevinteng/Desktop/ssd02/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii.gz'\n",
    "# #all brain affine are the same just pick one \n",
    "# brain_affine = nib.load(actual_label).affine\n",
    "# steps = 1\n",
    "# acc_list = []\n",
    "# for train_or_val in sorted(os.listdir(ds)):\n",
    "#     save_dir = save_path + train_or_val+'_'+ver\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     merge01 = os.path.join(ds+train_or_val)\n",
    "#     for patient in sorted(os.listdir(merge01)):\n",
    "#         patient_id = patient.split('.')[0]\n",
    "#         merge02 = os.path.join(merge01,patient)\n",
    "#         imgs = np.load(merge02)\n",
    "#         image = imgs[:,:,:,:4]\n",
    "#         seg_output = 0 #flush RAM\n",
    "#         seg_output = np.zeros((240,240,155))\n",
    "#         for i in range(image.shape[0]):\n",
    "#             inp = tf.expand_dims(image[i],0)\n",
    "#             img_seg = output_fn(inp) #validation function \n",
    "#             #map from sparse to label\n",
    "#             seg_output[:,:,i] = np.argmax(img_seg,-1) \n",
    "#         #convert label from 4 to 3 and np array and cast as int\n",
    "#         seg_output= np.where(seg_output==3,4,seg_output).astype(np.uint8)\n",
    "#         prediction_ni = nib.Nifti1Image(seg_output, brain_affine)\n",
    "#         prediction_ni.to_filename(save_dir+'/{}.nii.gz'.format(patient_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 240, 240, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coord_conv (CoordConv)          (None, 240, 240, 64) 3520        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv02 (SeparableConv2D)        (None, 240, 240, 128 8896        coord_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 240, 240, 128 0           conv02[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv03 (SeparableConv2D)        (None, 240, 240, 64) 9408        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 240, 240, 64) 0           conv03[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add01 (Add)                     (None, 240, 240, 64) 0           activation_11[0][0]              \n",
      "                                                                 coord_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 120, 1)  577         add01[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "coord_conv_1 (CoordConv)        (None, 120, 120, 128 3584        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 120, 120, 128 0           coord_conv_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv04 (SeparableConv2D)        (None, 120, 120, 256 34176       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 120, 120, 256 0           conv04[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv05 (SeparableConv2D)        (None, 120, 120, 128 35200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 120, 120, 128 0           conv05[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add02 (Add)                     (None, 120, 120, 128 0           activation_14[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 1)    1153        add02[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "coord_conv_2 (CoordConv)        (None, 60, 60, 256)  7168        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 60, 60, 256)  0           coord_conv_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv06 (SeparableConv2D)        (None, 60, 60, 512)  133888      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 60, 60, 512)  0           conv06[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv07 (SeparableConv2D)        (None, 60, 60, 256)  135936      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 60, 60, 256)  0           conv07[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add03 (Add)                     (None, 60, 60, 256)  0           activation_17[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 373,506\n",
      "Trainable params: 373,506\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}