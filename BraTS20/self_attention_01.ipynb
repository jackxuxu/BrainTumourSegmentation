{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/home/kevinteng/Desktop/BrainTumourSegmentation')\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "import utils\n",
    "from utils_vis import plot_comparison, plot_labels_color \n",
    "from utils import dice_coef, ss_metric, compute_metric\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue => Label 1 (Necrotic and Non-enhancing Tumor Core)\n",
    "- Yellow => Label 2 (Peritumoral Edema)\n",
    "- Green => Label 3/4 (GD-Enhancing Tumor)\n",
    "---\n",
    "* Core => Label 1 & 3\n",
    "* Enhancing => Label 3\n",
    "* Complete => Label 1,2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 4000\n",
    "max_epochs = 2\n",
    "BATCH_SIZE = 24\n",
    "lr = 0.001\n",
    "opt = tf.keras.optimizers.Adam(lr)\n",
    "ver = 'self_attention_01' #save version \n",
    "dropout=0.3 #dropout rate\n",
    "hn = 'he_normal' #kernel initializer \n",
    "tfrecords_read_dir = '/home/kevinteng/Desktop/ssd02/BraTS20_tfrecords03/'\n",
    "stack_npy = \"/home/kevinteng/Desktop/ssd02/BraTS2020_stack03/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "def generalized_dice(y_true, y_pred, smooth = 1e-5):\n",
    "    \"\"\"\n",
    "    Generalized Dice Score\n",
    "    https://arxiv.org/pdf/1707.03237\n",
    "    https://github.com/Mehrdad-Noori/Brain-Tumor-Segmentation/blob/master/loss.py\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true    = tf.reshape(y_true,shape=(-1,4))\n",
    "    y_pred    = tf.reshape(y_pred,shape=(-1,4))\n",
    "    sum_p     = tf.reduce_sum(y_pred, -2)\n",
    "    sum_r     = tf.reduce_sum(y_true, -2)\n",
    "    sum_pr    = tf.reduce_sum(y_true * y_pred, -2)\n",
    "    weights   = tf.math.pow(tf.math.square(sum_r) + smooth, -1)\n",
    "    generalized_dice = (2 * tf.reduce_sum(weights * sum_pr)) / (tf.reduce_sum(weights * (sum_r + sum_p)))\n",
    "    return generalized_dice\n",
    "\n",
    "def generalized_dice_loss(y_true, y_pred):   \n",
    "    return 1-generalized_dice(y_true, y_pred)\n",
    "    \n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    The final loss function consists of the summation of two losses \"GDL\" and \"CE\"\n",
    "    with a regularization term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return generalized_dice_loss(y_true, y_pred) + 1.25 * xent(y_true, y_pred)\n",
    "\n",
    "def data_aug(imgs, seed=8888):\n",
    "    x = tf.image.random_flip_up_down(imgs,seed)\n",
    "    x = tf.image.random_flip_left_right(x,seed)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Layer Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# template for guided attention block\n",
    "layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                  'pam01_alpha','pam01_add']\n",
    "layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                  'pam02_alpha', 'pam02_add']\n",
    "layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "layer_name_template = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "layer_name_ga = []\n",
    "for b in range(1,4):\n",
    "    layer_block = []\n",
    "    for layer in layer_name_template:\n",
    "        layer_internal = [i+'block0{}'.format(b) for i in layer]\n",
    "        layer_block.append(layer_internal)\n",
    "    layer_name_ga.append(layer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import conv_block, coordconv_block, up, pool, attention_block\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Activation, Add, Multiply, GaussianNoise\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, Dropout, Softmax, concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Layer, Softmax\n",
    "from coord_conv import CoordConv\n",
    "\n",
    "def conv_block_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert type(filters)==list, \"Please input filters of type list.\"\n",
    "    assert type(layer_name)==list, \"Please input filters of type list.\"\n",
    "    x = SeparableConv2D(filters[0], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[0])(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    x = SeparableConv2D(filters[1], kernel_size, padding='same', kernel_initializer=kernel_initializer, name = layer_name[1])(x)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = SeparableConv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def conv_2d(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None):\n",
    "    x = Conv2D(filters, kernel_size, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def down_sampling_sep(x_in, filters, layer_name, batch_norm=False, kernel_size=(3, 3), acti='relu',\n",
    "            kernel_initializer='glorot_uniform', dropout_rate=None, mode ='coord', x_dim=None, y_dim=None):\n",
    "    assert mode=='coord' or mode=='normal', \"Use 'coord' or 'normal' for mode!\"\n",
    "    if mode=='coord':\n",
    "        #seperable coordconv\n",
    "        assert (x_dim!=None and y_dim!=None), \"Please input dimension for CoordConv!\"\n",
    "        x = Conv2D(1, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer)(x_in)\n",
    "        x = CoordConv(x_dim=x_dim, y_dim=y_dim, with_r=False, filters=filters, strides=(1,1), \n",
    "                      kernel_size = 3, padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x)\n",
    "    else:\n",
    "        #normal mode\n",
    "        x = SeparableConv2D(filters, kernel_size, strides=(2, 2), padding='same', kernel_initializer=kernel_initializer, name=layer_name)(x_in)\n",
    "    if batch_norm == True:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(acti)(x)\n",
    "    if dropout_rate != None:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def res_block_sep(x_in, filters,  layer_name, batch_norm=False, kernel_size=(3, 3),\n",
    "               kernel_initializer='glorot_uniform', acti='relu', dropout_rate=None):\n",
    "    assert len(filters)==2, \"Please assure that there is 3 values for filters.\"\n",
    "    assert len(layer_name)==3, \"Please assure that there is 3 values for layer name\"\n",
    "    layer_name_conv = [layer_name[i] for i in range(len(layer_name)-1)]\n",
    "    output_conv_block = conv_block_sep(x_in, filters, layer_name_conv, batch_norm=batch_norm, kernel_size=kernel_size,\n",
    "                                   kernel_initializer = kernel_initializer, acti = acti, dropout_rate=dropout_rate)\n",
    "    output_add = Add(name = layer_name[-1])([output_conv_block, x_in])\n",
    "    return output_add\n",
    "\n",
    "def attention_block(input_signal, gated_signal, filters, att_layer_name):\n",
    "    #input signal feature maps\n",
    "    is_fm = Conv2D(filters, kernel_size=(1,1), strides=(2, 2), padding = 'same')(input_signal)\n",
    "    #gated signal feature maps\n",
    "    gs_fm = Conv2D(filters, kernel_size=(1,1), strides=(1, 1), padding = 'same')(gated_signal)\n",
    "    #debugger\n",
    "    assert is_fm.shape!=gs_fm.shape, \"Feature maps shape doesn't match!\"\n",
    "    #element wise sum\n",
    "    add = Add()([is_fm, gs_fm])\n",
    "    acti = Activation('relu')(add)\n",
    "    #downsampled attention coefficient\n",
    "    bottle_neck = Conv2D(1, kernel_size=(1,1), activation='sigmoid', name=att_layer_name)(acti)\n",
    "    #bilinear interpolation to get attention coeffcient\n",
    "    alpha = UpSampling2D(interpolation='bilinear')(bottle_neck)\n",
    "    #filter off input signal's features with attention coefficient\n",
    "    multi = Multiply()([input_signal, alpha])\n",
    "    return multi\n",
    "\n",
    "# class att_var(Layer):\n",
    "#     '''\n",
    "#     Attention variable\n",
    "#     '''\n",
    "#     def __init__(self, initial_val):\n",
    "#         super(att_var, self).__init__()\n",
    "#         self.initial_val = initial_val\n",
    "#     def __call__(self):\n",
    "#         alpha = tf.Variable(initial_value=self.initial_val, trainable=True)\n",
    "#         return alpha\n",
    "\n",
    "def PAM(inp_feature, layer_name, kernel_initializer='glorot_uniform', acti='relu'):\n",
    "    '''\n",
    "    Position attention module\n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param layer_name: List of layer names\n",
    "    [1st conv block, 2nd conv block, softmax output, 3rd conv block, position coefficient, Add output]\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]\n",
    "    :return: PAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #scale down ratio\n",
    "    c_8 = c//8\n",
    "    #\n",
    "    assert len(layer_name)>=5, 'Layer list length should be 5!'\n",
    "    # Branch01 Dimension: [w,h,c/8] => [(wxh),c/8]\n",
    "    query = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[0], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "            kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    query = tf.reshape(query,[-1,(w*h),c_8 ])\n",
    "    # Branch02 Dimension: [w,h,c/8] => [c/8,(wxh)]\n",
    "    key = conv_2d(inp_feature, filters=c_8, layer_name=layer_name[1], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    key = tf.reshape(key, [-1,(w*h),c_8 ])\n",
    "    key = tf.einsum('bij->bji', key) #transpose/permutation\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0102 = tf.einsum('bij,bjk->bik', query, key) #[(wxh),(wxh)]\n",
    "    softmax0102 = Softmax(name=layer_name[2])(matmul_0102)\n",
    "    # Branch03\n",
    "    value = conv_2d(inp_feature, filters=c, layer_name=layer_name[3], batch_norm=False, kernel_size=(1, 1), acti=acti,\n",
    "        kernel_initializer=kernel_initializer, dropout_rate=None)\n",
    "    value = tf.reshape(value,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('bij,bjk->bik',softmax0102,value) #[(wxh),c]\n",
    "    # Output\n",
    "    output = tf.reshape(matmul_all, [-1,w,h,c]) #[w,h,c]\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    lambda_p = Conv2D(filters=1,kernel_size=1,padding='same',activation='sigmoid', name=layer_name[4])(inp_feature)\n",
    "    output = Multiply()([output, lambda_p])\n",
    "    output_add = Add(name = layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def CAM(inp_feature, layer_name):\n",
    "    '''\n",
    "    Channel attention module \n",
    "    by default input shape => [w,h,c],[240, 240, 128] hence c/8 = 16\n",
    "    :param inp_feature: feature maps of res block after up sampling [w,h,c]k\n",
    "    :param layer_name: List of layer names\n",
    "        [softmax output, channel attention coefficients, Add output]\n",
    "    :return: CAM features [w,h,c] *dimension same as input!\n",
    "    '''\n",
    "    #dimensions\n",
    "    b,w,h,c = inp_feature.shape\n",
    "    #learnable coefficient to control the importance of CAM\n",
    "    assert len(layer_name)>=2, 'Layer list length should be 2!'\n",
    "    # Branch01 Dimension: [w,h,c] => [(wxh),c]\n",
    "    query = tf.reshape(inp_feature, [-1,(w*h),c])\n",
    "    # Branch02 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    key = tf.reshape(inp_feature, [-1,(w*h),c]) #[(wxh),c]\n",
    "    key = tf.einsum('ijk->ikj', key) #Permute:[c,(wxh)]\n",
    "    #matmul pipeline 01 & 02\n",
    "    matmul_0201 = tf.einsum('ijk,ikl->ijl', key, query) #[c,c]\n",
    "    softmax0102 = Softmax(name=layer_name[0])(matmul_0201)\n",
    "    # Branch03 Dimension: [w,h,c] => [c,(wxh)]\n",
    "    value = tf.reshape(inp_feature,[-1,(w*h),c]) #[(wxh),c]\n",
    "    matmul_all = tf.einsum('ijk,ikl->ijl', value,softmax0102) #[(wxh),c]\n",
    "    #output\n",
    "    output = tf.reshape(matmul_all,[-1,w,h,c])#[w,h,c]\n",
    "    #provides learnable parameter\n",
    "    #*channel wise attention, inspired by Squeeze Excitation(SE) block\n",
    "    GAP = GlobalAveragePooling2D()(output)\n",
    "    dense01 = Dense(c//8, activation='relu')(GAP)\n",
    "    alpha_c = Dense(c,activation='sigmoid', name=layer_name[1])(dense01)\n",
    "    #outputs\n",
    "    output = Multiply()([output, alpha_c])\n",
    "    output_add = Add(name=layer_name[-1])([output, inp_feature])\n",
    "    return output_add\n",
    "\n",
    "def guided_attention_block(inp_feature, layer_name_p, layer_name_c):\n",
    "    '''\n",
    "    Guided attention block that takes feature as input and concatenates features\n",
    "    from PAM and CAM as output\n",
    "    :param inp_feature: Input features\n",
    "    :param layer_name_p: layer name list for PAM\n",
    "    :param layer_name_c: layer name list for CAM\n",
    "    :return: squeezed concatenated features of PAM and CAM\n",
    "    '''\n",
    "    pam_feature = PAM(inp_feature, layer_name_p, kernel_initializer=hn)\n",
    "    cam_feature = CAM(inp_feature, layer_name_c)\n",
    "    add = Add()([pam_feature,cam_feature])\n",
    "    squeeze = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn,\n",
    "                       activation='relu')(add)\n",
    "    return squeeze\n",
    "\n",
    "def guided_attention(res_feature, ms_feature, layer_name):\n",
    "    '''\n",
    "    Guided attention module\n",
    "    :param res_feature: Upsampled Feature maps from Res Block\n",
    "    :param ms_feature: Multi scale feature maps result from Res Block\n",
    "    :param layer_name: Layer Name should consist be a list contating 4 list\n",
    "    Example:\n",
    "    layer_name_p01 = ['pam01_conv01', 'pam01_conv02', 'pam01_softmax', 'pam01_conv03',\n",
    "                      'pam01_alpha','pam01_add']\n",
    "    layer_name_c01 = ['cam01_softmax', 'cam01_alpha','cam01_add']\n",
    "    layer_name_p02 = ['pam02_conv01', 'pam02_conv02', 'pam02_softmax', 'pam02_conv03',\n",
    "                      'pam02_alpha', 'pam02_add']\n",
    "    layer_name_c02 = ['cam02_softmax', 'cam02_alpha','cam02_add']\n",
    "    layer_name = [layer_name_p01, layer_name_c01, layer_name_p02, layer_name_c02]\n",
    "\n",
    "    :return: guided attention module with shape same as input\n",
    "    '''\n",
    "    assert len(layer_name)==4, \"Layer name should be a list consisting 4 lists!\"\n",
    "    #self attention block01\n",
    "    concat01 = concatenate([res_feature, ms_feature], axis=-1)\n",
    "    squeeze01 = guided_attention_block(concat01, layer_name[0], layer_name[1])\n",
    "    multi01 = Multiply()([squeeze01, ms_feature])\n",
    "    #self attention block02\n",
    "    concat02 = concatenate([multi01, res_feature], axis=-1)\n",
    "    squeeze02 = guided_attention_block(concat02, layer_name[2], layer_name[3])\n",
    "    return squeeze02\n",
    "\n",
    "def forward(x):\n",
    "    '''\n",
    "    Resnet as backbone for multiscale feature retrieval.\n",
    "    Each resblock output(input signal), next resblock output(gated signal) is\n",
    "    feed into the gated attention for multi scale feature refinement.\n",
    "    Each gated attention output is pass through a bottle neck layer to standardize\n",
    "    the channel size by squashing them to desired filter size of 64.\n",
    "    The features are upsampled at each block to the corresponding [wxh] dimension\n",
    "    of w:240, h:240.\n",
    "    The upsampled features are concat and squash to corresponding channel size of 64\n",
    "    which yield multiscale feature.\n",
    "    :param x: batched images\n",
    "    :return: feature maps of each res block\n",
    "    '''\n",
    "    #inject noise\n",
    "    gauss1 = GaussianNoise(0.01)(x)\n",
    "    #---- ResNet and Multiscale Features----\n",
    "    #1st block\n",
    "    conv01 = CoordConv(x_dim=240, y_dim=240, with_r=False, filters=64, strides=(1,1),\n",
    "                      kernel_size = 3, padding='same', kernel_initializer=hn, name='conv01')(gauss1)\n",
    "    res_block01 = res_block_sep(conv01, filters=[128, 64], layer_name=[\"conv02\", \"conv03\", \"add01\"])\n",
    "    #2nd block\n",
    "    down_01 = down_sampling_sep(res_block01, filters=128, layer_name = 'down_01',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=120, y_dim=120)\n",
    "    res_block02 = res_block_sep(down_01, filters=[256, 128], layer_name=[\"conv04\", \"conv05\", \"add02\"])\n",
    "    #3rd block\n",
    "    down_02 = down_sampling_sep(res_block02, filters=256, layer_name = 'down_02',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=60, y_dim=60)\n",
    "    res_block03 = res_block_sep(down_02, filters=[512, 256], layer_name=[\"conv06\", \"conv07\", \"add03\"])\n",
    "    #4th block\n",
    "    down_03 = down_sampling_sep(res_block03, filters=512, layer_name = 'down_03',  kernel_initializer=hn,\n",
    "                               mode='normal',x_dim=30, y_dim=30)\n",
    "    res_block04 = res_block_sep(down_03, filters=[1024, 512], layer_name=[\"conv08\", \"conv09\", \"add04\"])\n",
    "    #grid attention blocks\n",
    "    att_block01 = attention_block(res_block01,res_block02,64,'grid_att01')\n",
    "    att_block02 = attention_block(res_block02,res_block03,128,'grid_att02')\n",
    "    att_block03 = attention_block(res_block03, res_block04,256,'gird_att03')\n",
    "    #bottle neck => layer squash all attention block to same filter size 64\n",
    "    bottle01 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block01)\n",
    "    bottle02 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block02)\n",
    "    bottle03 = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(att_block03)\n",
    "    #upsampling for all layers to same (wxh) dimension=>240x240\n",
    "    up01 = bottle01 #[240,240,64]\n",
    "    up02 = UpSampling2D(size=(2, 2), interpolation='bilinear')(bottle02) #[120,120,64]=>[240,240,64]\n",
    "    up03 = UpSampling2D(size=(4,4), interpolation='bilinear')(bottle03) #[60,60,64]=>[240,240,64]\n",
    "    #multiscale features\n",
    "    concat_all = concatenate([up01,up02,up03],axis=-1) #[240,240,3*64]\n",
    "    #squeeze to have the same channel as upsampled features [240,240,3*64] => [240,240,64]\n",
    "    ms_feature = Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer=hn)(concat_all)\n",
    "    #Segmentations from multiscale features *without softmax activation\n",
    "    seg_01 = Conv2D(4, (1,1), name='seg_01')(up01)\n",
    "    seg_02 = Conv2D(4, (1,1), name='seg_02')(up02)\n",
    "    seg_03 = Conv2D(4, (1,1), name='seg_03')(up02)\n",
    "\n",
    "    #----self guided attention blocks-----\n",
    "    ga_01 = guided_attention(up01, ms_feature, layer_name_ga[0])\n",
    "    ga_02 = guided_attention(up02, ms_feature, layer_name_ga[1])\n",
    "    ga_03 = guided_attention(up03, ms_feature, layer_name_ga[2])\n",
    "    #Segmentations from guided attention features *without softmax activation\n",
    "    seg_ga01 = Conv2D(4, (1,1), name='seg_ga01')(ga_01)\n",
    "    seg_ga02 = Conv2D(4, (1,1), name='seg_ga02')(ga_02)\n",
    "    seg_ga03 = Conv2D(4, (1,1), name='seg_ga03')(ga_03)\n",
    "    #outputs for xent losses\n",
    "    output_xent = [seg_01, seg_02, seg_03, seg_ga01, seg_ga02, seg_ga03]\n",
    "    #output for dice coefficient loss\n",
    "    pred_seg = Add()(output_xent)\n",
    "    pred_seg_mean = pred_seg/len(output_xent)\n",
    "    output_dice = Softmax()(pred_seg_mean)\n",
    "    return output_xent, output_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Build Model\n",
    "input_layer = Input(shape=(240,240,4))\n",
    "model = Model(input_layer, forward(input_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_logit = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_fn(image, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_xent, output_dice = model(image, training=True)\n",
    "        loss_dice = generalized_dice_loss(label, output_dice)\n",
    "        loss_xents=[]\n",
    "        for seg in output_xent:\n",
    "            loss_xent = xent_logit(label, seg)\n",
    "            loss_xents.append(loss_xent)\n",
    "        loss_total = sum(loss_xents)+loss_dice\n",
    "    gradients = tape.gradient(loss_total, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return output_dice, loss_total, gradients\n",
    "\n",
    "@tf.function\n",
    "def val_fn(image, label):\n",
    "    model_output = model(image, training=False)\n",
    "    loss = custom_loss(label, model_output)\n",
    "    return model_output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epochs  1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[1,57600,57600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_op_layer_Einsum_25/Einsum_25 (defined at <ipython-input-7-7eaa56edc150>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_fn_11233]\n\nFunction call stack:\ntrain_fn\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-70abeddcc812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mimg_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0;31m#map from sparse to label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mimg_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_seg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1,57600,57600] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_op_layer_Einsum_25/Einsum_25 (defined at <ipython-input-7-7eaa56edc150>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_fn_11233]\n\nFunction call stack:\ntrain_fn\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "#list\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "loss_inner = []\n",
    "while epochs <= max_epochs:\n",
    "    start = time.time()\n",
    "    print()\n",
    "    print(\"Epochs {:2d}\".format(epochs))\n",
    "    steps = 1\n",
    "    dc_app = []\n",
    "    sens_app = []\n",
    "    spec_app = []\n",
    "    ds = os.listdir(tfrecords_read_dir)\n",
    "    #shuffle directory list of tfrecords\n",
    "    shuffle = random.shuffle(ds)\n",
    "    for tf_re in ds:\n",
    "        tf_dir = os.path.join(tfrecords_read_dir+tf_re)\n",
    "        dataset = utils.parse_tfrecord(tf_dir).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "        acc_inner = []\n",
    "        for imgs in dataset:\n",
    "            #data augmentation\n",
    "            imgs = data_aug(imgs)\n",
    "            image = imgs[:,:,:,:4]\n",
    "            #unprocessed label for plotting\n",
    "            label = imgs[:,:,:,-1]\n",
    "            #for simplicity label 4 will be converted to 3 for sparse encoding\n",
    "            label = tf.where(label==4,3,label)\n",
    "            label = tf.keras.utils.to_categorical(label, num_classes=4)\n",
    "            img_seg, loss, gradients = train_fn(image, label) #training function\n",
    "            #map from sparse to label\n",
    "            img_seg = tf.math.argmax(img_seg,-1,output_type=tf.int32)\n",
    "            label = tf.math.argmax(label,-1,output_type=tf.int32)\n",
    "            #accuracy of the output values for that batch\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(img_seg,label), tf.float32))\n",
    "            #append accuracy for every steps\n",
    "            acc_inner.append(acc)\n",
    "            if epochs%5==0:\n",
    "                dc_list, sens_list, spec_list =compute_metric(label,img_seg)\n",
    "                dc_app.append(dc_list)\n",
    "                sens_app.append(sens_list)\n",
    "                spec_app.append(spec_list)\n",
    "            #output\n",
    "            if steps%1==0:\n",
    "                input_img = [image[0,:,:,0], plot_labels_color(label[0]), plot_labels_color(img_seg[0])]\n",
    "                caption = ['Input Image', 'Ground Truth', 'Model Output']\n",
    "                plot_comparison(input_img, caption, n_col = 3, figsize=(10,10))\n",
    "                loss_list.append(loss)\n",
    "                acc_stp = tf.reduce_mean(tf.cast(tf.equal(img_seg[0],label[0]), tf.float32))\n",
    "                dc_list_stp, sens_list_stp, spec_list_stp =compute_metric(label[0],img_seg[0])\n",
    "                print(\"Steps: {}, Loss:{}\".format(steps, loss))\n",
    "                print(\"Accurary: {}\".format(acc_stp))\n",
    "                print(\"Dice coefficient: {}\".format(dc_list_stp))\n",
    "                print(\"Sensitivity: {}\".format(sens_list_stp))\n",
    "                print(\"Specificity: {}\".format(spec_list_stp))\n",
    "                print(\"Gradient min:{}, max:{}\".format(np.min(gradients[0]), np.max(gradients[0])))\n",
    "            steps+=1\n",
    "        acc_list.append(np.mean(acc_inner))\n",
    "    if epochs%5==0:\n",
    "        mean_dc = np.mean(np.array(dc_app),0)\n",
    "        mean_sens = np.mean(np.array(sens_app),0)\n",
    "        mean_spec = np.mean(np.array(spec_app),0)\n",
    "        print()\n",
    "        print('-----------<Summary for Epoch:{}>------------'.format(epochs))\n",
    "        print(\"Mean Accuracy: {}\".format(np.mean(acc_list)))\n",
    "        #'core','enhancing','complete'\n",
    "        print(\"Mean Dice coefficient: {}\".format(mean_dc))\n",
    "        print(\"Mean Sensitivity: {}\".format(mean_sens))\n",
    "        print(\"Mean Specificity: {}\".format(mean_spec))\n",
    "        print('------------------------------------------------')\n",
    "        print()\n",
    "    elapsed_time =(time.time()-start)/60 #unit in mins\n",
    "    print(\"Compute time per epochs: {:.2f} mins\".format(elapsed_time))\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/home/kevinteng/Desktop/model_weights/model_{}.h5'.format(ver))\n",
    "def output_fn(image):\n",
    "    model.trainable = False\n",
    "    model_output = model(image)\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ds = '/home/kevinteng/Desktop/ssd02/BraTS2020_preprocessed03/'\n",
    "# save_path = '/home/kevinteng/Desktop/ssd02/submission/'\n",
    "# actual_label = '/home/kevinteng/Desktop/ssd02/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii.gz'\n",
    "# #all brain affine are the same just pick one \n",
    "# brain_affine = nib.load(actual_label).affine\n",
    "# steps = 1\n",
    "# acc_list = []\n",
    "# for train_or_val in sorted(os.listdir(ds)):\n",
    "#     save_dir = save_path + train_or_val+'_'+ver\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     merge01 = os.path.join(ds+train_or_val)\n",
    "#     for patient in sorted(os.listdir(merge01)):\n",
    "#         patient_id = patient.split('.')[0]\n",
    "#         merge02 = os.path.join(merge01,patient)\n",
    "#         imgs = np.load(merge02)\n",
    "#         image = imgs[:,:,:,:4]\n",
    "#         seg_output = 0 #flush RAM\n",
    "#         seg_output = np.zeros((240,240,155))\n",
    "#         for i in range(image.shape[0]):\n",
    "#             inp = tf.expand_dims(image[i],0)\n",
    "#             img_seg = output_fn(inp) #validation function \n",
    "#             #map from sparse to label\n",
    "#             seg_output[:,:,i] = np.argmax(img_seg,-1) \n",
    "#         #convert label from 4 to 3 and np array and cast as int\n",
    "#         seg_output= np.where(seg_output==3,4,seg_output).astype(np.uint8)\n",
    "#         prediction_ni = nib.Nifti1Image(seg_output, brain_affine)\n",
    "#         prediction_ni.to_filename(save_dir+'/{}.nii.gz'.format(patient_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
